{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYDL_dCEoEdH",
        "outputId": "96e7e591-2e87-48ab-ba45-26efff46262e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import wandb\n",
        "import json\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Seeds for reproducibility\n",
        "def set_seed(seed: int = 123):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "set_seed(123)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd1AgVLg-UaB"
      },
      "outputs": [],
      "source": [
        "\"\"\" PLOT SETTINGS \"\"\"\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "plt.rcParams.update({\n",
        "    \"font.size\": 18,            # base font size\n",
        "    \"axes.titlesize\": 24,       # axis titles\n",
        "    \"axes.labelsize\": 22,       # axis labels\n",
        "    \"xtick.labelsize\": 18,      # X axis numbers\n",
        "    \"ytick.labelsize\": 18,      # Y axis numbers\n",
        "    \"legend.fontsize\": 20,      # legend text\n",
        "    \"lines.linewidth\": 3.0      # line thickness\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RH1Cth5Svef"
      },
      "outputs": [],
      "source": [
        "\"\"\" CLASS TO CREATE THE HEAD with 100 CLASSES \"\"\"\n",
        "class DINOWithHead(nn.Module):\n",
        "    def __init__(self, backbone, num_classes=100, p=None):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        layers = []\n",
        "        if p is not None:\n",
        "            layers.append(nn.Dropout(p=p))\n",
        "        layers.append(nn.Linear(384, num_classes))\n",
        "        self.head = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        return self.head(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJtMX7-9Z5_D"
      },
      "source": [
        "### Dataset Download and Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyyFR5pwqFuv"
      },
      "outputs": [],
      "source": [
        "\"\"\" DATASET DOWNLOAD \"\"\"\n",
        "\n",
        "ROOT = './data'\n",
        "BATCH_SIZE = 64\n",
        "#BATCH_SIZE = 128\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "tot_train_data = torchvision.datasets.CIFAR100(root=ROOT, train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
        "test_data = torchvision.datasets.CIFAR100(root=ROOT, train=False, download=True, transform=torchvision.transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvBGsuJ9qMeY"
      },
      "outputs": [],
      "source": [
        "\"\"\" SPLIT TOT_TRAININ in VALIDATION and TRAIN \"\"\"\n",
        "\n",
        "def split_dataset(tot_train_data, valid_ratio=0.8):\n",
        "    \"\"\"\n",
        "    Splits the given dataset randomly into training and validation subsets.\n",
        "    \"\"\"\n",
        "    train_size = int(valid_ratio * len(tot_train_data))\n",
        "    val_size = len(tot_train_data) - train_size\n",
        "    train_data, val_data = random_split(tot_train_data, [train_size, val_size])\n",
        "    return train_data, val_data\n",
        "\n",
        "train_data, val_data = split_dataset(tot_train_data, valid_ratio=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnpYR9U0annj"
      },
      "outputs": [],
      "source": [
        "\"\"\" DATA TRANSFORMATION \"\"\"\n",
        "\n",
        "def data_trasform(dataset, data_augmentation=False):   ### train_data or tot_train_data\n",
        "    \"\"\"\n",
        "    Returns train and val/test transforms based on dataset stats.\n",
        "    Dataset (for computing mean and std) can be either training only or combined train+validation.\n",
        "\n",
        "    If data_augmentation=True, applies augmentation on training transforms, otherwise only resize and normalize.\n",
        "    \"\"\"\n",
        "\n",
        "    # MEAN and VARIANCE (considering 3 channels)\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "    nb_samples = 0\n",
        "\n",
        "    for img, _ in dataset:\n",
        "        img = img.view(3, -1)  # Flatten H*W in seconda dimensione\n",
        "        mean += img.mean(1)\n",
        "        std += img.std(1)\n",
        "        nb_samples += 1\n",
        "\n",
        "    mean /= nb_samples\n",
        "    std /= nb_samples\n",
        "\n",
        "\n",
        "    if data_augmentation:\n",
        "        train_transforms = transforms.Compose([\n",
        "            transforms.Resize(64, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "            transforms.RandomCrop(64, padding=4),\n",
        "            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(degrees=15),\n",
        "            transforms.RandAugment(num_ops=2, magnitude=9),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean, std=std)\n",
        "        ])\n",
        "    else:\n",
        "        train_transforms = transforms.Compose([\n",
        "            transforms.Resize(64, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean, std=std)\n",
        "        ])\n",
        "\n",
        "    ### NO DATA AUGMENTATION for val/test!\n",
        "    val_test_transforms = transforms.Compose([\n",
        "        transforms.Resize(64),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std)   # Normalization using the training statistics\n",
        "    ])\n",
        "\n",
        "\n",
        "    return train_transforms, val_test_transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY7RBmCoaQeT"
      },
      "outputs": [],
      "source": [
        "\"\"\" DATA TRANSFORMATION and LOADERS \"\"\"\n",
        "\n",
        "### ===== For hyperparameter tuning considering train_data and val_data =====\n",
        "train_transforms, val_test_transforms = data_trasform(train_data)\n",
        "\n",
        "train_data.dataset.transform = train_transforms\n",
        "val_data.dataset.transform = val_test_transforms\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "val_loader   = DataLoader(val_data,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "\n",
        "\n",
        "### ===== For model testing considering tot_train_data and test_data =====\n",
        "train_transforms, val_test_transforms = data_trasform(tot_train_data)\n",
        "\n",
        "tot_train_data = torchvision.datasets.CIFAR100(root=ROOT, train=True, download=False, transform=train_transforms)\n",
        "test_data = torchvision.datasets.CIFAR100(root=ROOT, train=False, download=False, transform=val_test_transforms)\n",
        "\n",
        "tot_train_loader = DataLoader(tot_train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImlDNxZRqdzk"
      },
      "source": [
        "### Test and Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2wJEmef0SdF"
      },
      "outputs": [],
      "source": [
        "\"\"\" TRAINING and TESTING \"\"\"\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion):\n",
        "    \"\"\"\n",
        "    The evaluate_model function computes the average loss and accuracy of a model on a dataset.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_corrects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            total_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader.dataset)\n",
        "    avg_acc = total_corrects.double() / len(data_loader.dataset)\n",
        "    return avg_loss, avg_acc.item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, scheduler, epoch, train_losses, train_accuracies,\n",
        "                    val_test_losses, val_test_accuracies, best_acc, best_loss, best_model_wts, path):\n",
        "    \"\"\"\n",
        "    The save_checkpoint function saves the model’s state, optimizer, scheduler, training/validation metrics,\n",
        "    and best performance to a specified file path.\n",
        "    \"\"\"\n",
        "    dir_name = os.path.dirname(path)\n",
        "    if dir_name:\n",
        "        os.makedirs(dir_name, exist_ok=True)\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
        "        'train_losses': train_losses,\n",
        "        'train_accuracies': train_accuracies,\n",
        "        'val_test_losses': val_test_losses,\n",
        "        'val_test_accuracies': val_test_accuracies,\n",
        "        'best_acc': best_acc,\n",
        "        'best_loss': best_loss,\n",
        "        'best_model_state_dict': best_model_wts\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def init_checkpoint(model, optimizer, scheduler, path=None, device='cpu'):\n",
        "    \"\"\"\n",
        "    Initialize a checkpoint. If path is None, create default checkpoint with empty/default values.\n",
        "    If path is given and file exists, load it.\n",
        "    \"\"\"\n",
        "    if path is None:  # default path\n",
        "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "        path = \"checkpoints/latest.pth\"\n",
        "        print(f\"Initializing new checkpoint at {path}\")\n",
        "        checkpoint = {   # save default empty checkpoint\n",
        "            'epoch': 1,\n",
        "            'best_acc': 0.0,\n",
        "            'best_loss': 1e10,\n",
        "            'train_losses': [],\n",
        "            'train_accuracies': [],\n",
        "            'val_test_losses': [],\n",
        "            'val_test_accuracies': [],\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
        "            'best_model_state_dict': copy.deepcopy(model.state_dict())\n",
        "        }\n",
        "        torch.save(checkpoint, path)\n",
        "        return 1, 0.0, 1e10, [], [], [], [], path, copy.deepcopy(model.state_dict())\n",
        "\n",
        "    else: # load existing checkpoint\n",
        "        if not os.path.isfile(path):\n",
        "            raise FileNotFoundError(f\"Checkpoint file {path} does not exist.\")\n",
        "\n",
        "        print(f\"Loading checkpoint from {path}\")\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        if scheduler and checkpoint.get('scheduler_state_dict'):\n",
        "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        best_model_wts = checkpoint.get('best_model_state_dict', copy.deepcopy(model.state_dict()))\n",
        "        return (checkpoint['epoch'],\n",
        "                checkpoint.get('best_acc', 0.0),\n",
        "                checkpoint.get('best_loss', 1e10),\n",
        "                checkpoint.get('train_losses', []),\n",
        "                checkpoint.get('train_accuracies', []),\n",
        "                checkpoint.get('val_test_losses', []),\n",
        "                checkpoint.get('val_test_accuracies', []),\n",
        "                path,\n",
        "                best_model_wts )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_test_model(model, criterion, optimizer, scheduler, train_loader, val_test_loader,\n",
        "                          num_epochs=10, checkpoint_path=None, checkpoints = True, verbose = 1):\n",
        "                                        # If checkpoint_path = None, a path is created and training starts from scratch\n",
        "                                        # If checkpoints = False, we don't save anything (used for the calibration part)\n",
        "    \"\"\"\n",
        "    Trains a model with logging and evaluation, returning the best model and metrics.\n",
        "    If a checkpoint path is provided, training will resume from the saved state in that file.\n",
        "    \"\"\"\n",
        "\n",
        "    since = time.time()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # === Checkpoints (Initialize if None, Load if it exists already) ===\n",
        "    if checkpoints: # Training with checkpoints\n",
        "        start_epoch, best_acc, best_loss, train_losses, train_accuracies, val_test_losses, val_test_accuracies, checkpoint_path, best_model_wts = \\\n",
        "            init_checkpoint(model, optimizer, scheduler, path=checkpoint_path, device=device)\n",
        "\n",
        "    else: # No checkpoint\n",
        "        start_epoch = 1\n",
        "        best_acc = 0.0\n",
        "        best_loss = 1e10\n",
        "        train_losses, train_accuracies, val_test_losses, val_test_accuracies = [], [], [], []\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # ===== Epoch loop =====\n",
        "    for epoch in range(start_epoch, num_epochs+1):\n",
        "        if checkpoints:\n",
        "            if verbose:\n",
        "                print(f'\\nEpoch {epoch}/{num_epochs}')\n",
        "                print('-' * 30)\n",
        "\n",
        "        # ===== Training =====\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_corrects = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            train_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        epoch_train_loss = train_loss / len(train_loader.dataset)\n",
        "        epoch_train_acc = train_corrects.double() / len(train_loader.dataset)\n",
        "\n",
        "        if len(train_losses) >= epoch:     # Overwriting the current epoch's results if resuming from this position (if the current loop was never compelted)\n",
        "            train_losses[epoch-1] = epoch_train_loss\n",
        "            train_accuracies[epoch-1] = epoch_train_acc.item()\n",
        "        else:\n",
        "            train_losses.append(epoch_train_loss)\n",
        "            train_accuracies.append(epoch_train_acc.item())\n",
        "\n",
        "        if verbose == True:  # Print each round\n",
        "            print(f'Train Loss: {epoch_train_loss:.4f} Train Acc: {epoch_train_acc:.4f}')\n",
        "\n",
        "\n",
        "        # ===== Validation/Test =====\n",
        "        epoch_val_test_loss, epoch_val_test_acc = evaluate_model(model, val_test_loader, criterion)\n",
        "\n",
        "        if len(val_test_losses) >= epoch:    # Overwriting the current epoch's results if resuming from this position\n",
        "            val_test_losses[epoch-1] = epoch_val_test_loss\n",
        "            val_test_accuracies[epoch-1] = epoch_val_test_acc\n",
        "        else:\n",
        "            val_test_losses.append(epoch_val_test_loss)\n",
        "            val_test_accuracies.append(epoch_val_test_acc)\n",
        "        if verbose == True:\n",
        "            print(f'Val/Test Loss: {epoch_val_test_loss:.4f}, Val/Test Acc: {epoch_val_test_acc:.4f}')\n",
        "\n",
        "        if verbose == 'mid':\n",
        "            if epoch == 1 or epoch % 5 == 0:   # Print occasionally\n",
        "                print(f\"Epoch {epoch}\")\n",
        "                print(f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}')\n",
        "                print(f'Val/Test Loss: {epoch_val_test_loss:.4f}, Val/Test Acc: {epoch_val_test_acc:.4f}')\n",
        "\n",
        "        # ===== Update and Save the best model =====\n",
        "        if epoch_val_test_acc > best_acc:\n",
        "            best_acc = epoch_val_test_acc\n",
        "            best_loss = epoch_val_test_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        # ===== Save checkpoint =====\n",
        "        if checkpoints:\n",
        "            save_checkpoint(\n",
        "                model, optimizer, scheduler,\n",
        "                epoch + 1,\n",
        "                train_losses, train_accuracies,\n",
        "                val_test_losses, val_test_accuracies,\n",
        "                best_acc, best_loss, best_model_wts,\n",
        "                checkpoint_path\n",
        "            )\n",
        "\n",
        "    # Training completed\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best Acc: {best_acc:.4f}, Best Loss: {best_loss:.4f}')\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model, train_losses, val_test_losses, train_accuracies, val_test_accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D04_fYWa6KG"
      },
      "source": [
        "# (1) CENTRALIZED MODEL - HEAD ONLY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf0cqc_DwuBa",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### (1.I) Hyperparameter Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeCzC2oJGC4t"
      },
      "outputs": [],
      "source": [
        "''' WANDB TRAINING '''\n",
        "\n",
        "def wandb_train(config=None):\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # === MODEL ===\n",
        "        dino_vits16 = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
        "        dino_vits16.eval()  # evaluation mode for the backbone\n",
        "        dino_vits16 = dino_vits16.to(device)\n",
        "\n",
        "        # Freeze backbone params\n",
        "        for param in dino_vits16.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Create the final model with trainable Head\n",
        "        model = DINOWithHead(dino_vits16, num_classes=100).to(device)\n",
        "        for param in model.head.parameters():\n",
        "            param.requires_grad = True\n",
        "        model.head.train()    # training mode for the head\n",
        "\n",
        "\n",
        "        # === LOSS, OPTIMIZER, SCHEDULER ===\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.SGD(\n",
        "              model.parameters(),\n",
        "              lr=config.lr,\n",
        "              momentum=config.momentum,\n",
        "              weight_decay=config.weight_decay\n",
        "        )\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.t_max)\n",
        "\n",
        "        # === TRAINING ===\n",
        "        model, train_losses, val_losses, train_accs, val_accs = train_test_model(\n",
        "            model,\n",
        "            criterion,\n",
        "            optimizer,\n",
        "            scheduler,\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            num_epochs=config.epochs,\n",
        "            verbose='mid'\n",
        "        )\n",
        "\n",
        "\n",
        "        # === LOG ===\n",
        "        for epoch in range(config.epochs):\n",
        "            wandb.log({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"train_loss\": train_losses[epoch],\n",
        "                \"val_loss\": val_losses[epoch] if val_losses else None,\n",
        "                \"train_accuracy\": train_accs[epoch],\n",
        "                \"val_accuracy\": val_accs[epoch] if val_accs else None,\n",
        "            })\n",
        "\n",
        "        # === BEST LOG ===\n",
        "        if val_accs:\n",
        "            best_idx = val_accs.index(max(val_accs))\n",
        "            wandb.run.summary[\"best_val_accuracy\"] = val_accs[best_idx]\n",
        "            wandb.run.summary[\"best_val_loss\"] = val_losses[best_idx]\n",
        "            wandb.run.summary[\"best_train_accuracy\"] = train_accs[best_idx]\n",
        "            wandb.run.summary[\"best_train_loss\"] = train_losses[best_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7BGzib1X_F7",
        "outputId": "d280d564-0ded-4e85-ce77-0a109ab1f167"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgabriele_\u001b[0m (\u001b[33mgabriele-politecnico-di-torino\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: 0wz9r8de\n",
            "Sweep URL: https://wandb.ai/gabriele-politecnico-di-torino/Project_Central_Grid/sweeps/0wz9r8de\n"
          ]
        }
      ],
      "source": [
        "wandb.login()\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'bayes',    # probabilistic model (based on previous results, it predicts which hyperparameter combinations are likely to lead to better performance)\n",
        "    'metric': {\n",
        "        'name': 'val_loss',\n",
        "        'goal': 'minimize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'lr': {\n",
        "            'distribution': 'log_uniform_values',   # log_uniform as numbers are small\n",
        "            'min': 0.0001,\n",
        "            'max': 0.001\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'value': 64\n",
        "        },\n",
        "        'momentum': {\n",
        "            'distribution': 'uniform',\n",
        "            'min': 0.7,\n",
        "            'max': 0.9\n",
        "        },\n",
        "        'dropout': {\n",
        "            'distribution': 'uniform',\n",
        "            'min': 0.0,\n",
        "            'max': 0.1\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'distribution': 'log_uniform_values',    # log_uniform as numbers are small\n",
        "            'min': 1e-6,\n",
        "            'max': 1e-5\n",
        "        },\n",
        "        'epochs': {\n",
        "            'value': 10\n",
        "        },\n",
        "        't_max': {\n",
        "            'value': 10   # same as n_epochs\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"Project_Central_Grid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMasDyU2YFxY"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, function=wandb_train, count=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK5oNEQKfaBW"
      },
      "source": [
        "### (1.II) Final Model (with best hyperparameters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9jBIOzpSvei"
      },
      "outputs": [],
      "source": [
        "set_seed(seed=123)\n",
        "N_EP = 40\n",
        "T_MAX = N_EP\n",
        "LR = 0.001\n",
        "MOMENTUM = 0.8\n",
        "WEIGHT_DECAY = 5e-6\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# === MODEL ===\n",
        "dino_vits16 = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
        "dino_vits16.eval()  # evaluation mode for the backbone\n",
        "dino_vits16 = dino_vits16.to(device)\n",
        "\n",
        "# Freeze backbone params\n",
        "for param in dino_vits16.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Create the final model (trainable head)\n",
        "model_central = DINOWithHead(dino_vits16, num_classes=100).to(device)\n",
        "for param in model_central.head.parameters():\n",
        "            param.requires_grad = True\n",
        "model_central.head.train()    # training mode for the head\n",
        "\n",
        "# === LOSS, OPTIMIZER, SCHEDULER ===\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(\n",
        "      model_central.head.parameters(), # We optimize only the head\n",
        "      lr=LR,\n",
        "      momentum=MOMENTUM,\n",
        "      weight_decay=WEIGHT_DECAY\n",
        "      )\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_MAX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XigJ0EK0SdJ"
      },
      "outputs": [],
      "source": [
        "set_seed(seed=123)\n",
        "\n",
        "# === MODEL TRAINING and TESTING with logging & checkpointing ===\n",
        "#checkpoint_path = \"checkpoints/latest.pth\"     # It will start from where it stopped and \"re-do\" the last round if it was incomplete\n",
        "checkpoint_path = None\n",
        "\n",
        "start_time = time.time()\n",
        "final_model, train_losses, test_losses, train_accuracies, test_accuracies = train_test_model(\n",
        "    model_central,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    tot_train_loader,\n",
        "    val_test_loader=test_loader,\n",
        "    num_epochs=N_EP,\n",
        "    checkpoint_path=checkpoint_path,\n",
        ")\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEMW6HjU0SdK"
      },
      "outputs": [],
      "source": [
        "# === SAVING RESULTS ===\n",
        "results = {\n",
        "    \"epochs\": N_EP,\n",
        "    \"train_losses\": train_losses,\n",
        "    \"train_accuracies\": train_accuracies,\n",
        "    \"test_losses\": test_losses,\n",
        "    \"test_accuracies\": test_accuracies,\n",
        "    \"time_sec\": round(elapsed_time, 2)\n",
        "}\n",
        "print(results)\n",
        "\n",
        "json_filename = \"results_central_BEST.json\"\n",
        "\n",
        "with open(json_filename, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "torch.save(final_model.state_dict(), \"final_model_weights_BEST.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J5I0gmV0SdK"
      },
      "outputs": [],
      "source": [
        "# === PLOTTING RESULTS ===\n",
        "epochs = range(1, N_EP+1)\n",
        "\n",
        "# === Plot Loss ===\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.plot(epochs, train_losses, label=\"Training Loss\", marker=\"o\")\n",
        "if test_losses is not None:\n",
        "    plt.plot(epochs, test_losses, label=\"Test Loss\", marker=\"s\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs Test Loss\", fontsize=26, fontweight=\"bold\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# === Plot Accuracy ===\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", marker=\"o\")\n",
        "if test_accuracies is not None:\n",
        "    plt.plot(epochs, test_accuracies, label=\"Test Accuracy\", marker=\"s\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training vs Test Accuracy\", fontsize=26, fontweight=\"bold\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}