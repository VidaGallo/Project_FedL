{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84300387",
      "metadata": {
        "id": "84300387"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import Subset\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from collections import OrderedDict\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "import wandb\n",
        "import json\n",
        "import heapq\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "\n",
        "# Seeds for reproducibility\n",
        "def set_seeds(seed: int = 123):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seeds(123)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7f9f76a6",
      "metadata": {
        "id": "7f9f76a6"
      },
      "outputs": [],
      "source": [
        "\"\"\" PLOT SETTINGS \"\"\"\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "plt.rcParams.update({\n",
        "    \"font.size\": 18,            # base font size\n",
        "    \"axes.titlesize\": 24,       # axis titles\n",
        "    \"axes.labelsize\": 22,       # axis labels\n",
        "    \"xtick.labelsize\": 18,      # X axis numbers\n",
        "    \"ytick.labelsize\": 18,      # Y axis numbers\n",
        "    \"legend.fontsize\": 18,      # legend text\n",
        "    \"lines.linewidth\": 3.0      # line thickness\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "85eba51c",
      "metadata": {
        "id": "85eba51c"
      },
      "outputs": [],
      "source": [
        "# --- Define DINOWithHead ---\n",
        "class DINOWithHead(nn.Module):\n",
        "    def __init__(self, backbone, num_classes=100, p=None):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        layers = []\n",
        "        if p is not None:\n",
        "            layers.append(nn.Dropout(p=p))\n",
        "        layers.append(nn.Linear(384, num_classes))\n",
        "        self.head = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        out = self.head(features)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a50b02b4",
      "metadata": {
        "id": "a50b02b4"
      },
      "source": [
        "## Data import and loader creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0326f737",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0326f737",
        "outputId": "1d0ff9ec-a8bd-4439-a359-fc69cdded26c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "\"\"\" DATASET DOWNLOAD \"\"\"\n",
        "ROOT = './data'\n",
        "BATCH_SIZE=64\n",
        "#BATCH_SIZE = 128\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "tot_train_data = torchvision.datasets.CIFAR100(root=ROOT, train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
        "test_data = torchvision.datasets.CIFAR100(root=ROOT, train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" SPLIT TOT_TRAININ in VALIDATION and TRAIN \"\"\"\n",
        "\n",
        "def split_dataset(tot_train_data, valid_ratio=0.8):\n",
        "    \"\"\"\n",
        "    Splits the given dataset randomly into training and validation subsets\n",
        "    \"\"\"\n",
        "    train_size = int(valid_ratio * len(tot_train_data))\n",
        "    val_size = len(tot_train_data) - train_size\n",
        "    train_data, val_data = random_split(tot_train_data, [train_size, val_size])\n",
        "    return train_data, val_data\n",
        "\n",
        "train_data, val_data = split_dataset(tot_train_data, valid_ratio=0.8)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" DATA TRANSFORMATION \"\"\"\n",
        "\n",
        "def data_trasform(dataset, data_augmentation=False):   ### train_data or tot_train_data\n",
        "    \"\"\"\n",
        "    Returns train and val/test transforms based on dataset stats.\n",
        "    Dataset (for computing mean and std) can be either training only or combined train+validation.\n",
        "\n",
        "    If data_augmentation=True, applies augmentation on training transforms, otherwise only resize and normalize.\n",
        "    \"\"\"\n",
        "\n",
        "    # MEAN and VARIANCE (considering 3 channels)\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "    nb_samples = 0\n",
        "\n",
        "    for img, _ in dataset:\n",
        "        img = img.view(3, -1)  # Flatten H*W in seconda dimensione\n",
        "        mean += img.mean(1)\n",
        "        std += img.std(1)\n",
        "        nb_samples += 1\n",
        "\n",
        "    mean /= nb_samples\n",
        "    std /= nb_samples\n",
        "\n",
        "\n",
        "    if data_augmentation:\n",
        "        train_transforms = transforms.Compose([\n",
        "            transforms.Resize(64, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "            transforms.RandomCrop(64, padding=4),\n",
        "            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(degrees=15),\n",
        "            transforms.RandAugment(num_ops=2, magnitude=9),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean, std=std)\n",
        "        ])\n",
        "    else:\n",
        "        train_transforms = transforms.Compose([\n",
        "            transforms.Resize(64, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean, std=std)\n",
        "        ])\n",
        "\n",
        "    ### NO DATA AUGMENTATION!\n",
        "    val_test_transforms = transforms.Compose([\n",
        "        transforms.Resize(64),                    # Resize\n",
        "        transforms.ToTensor(),                     # Convert to tensor\n",
        "        transforms.Normalize(mean=mean, std=std)   # Normalization using the training statistics\n",
        "    ])\n",
        "\n",
        "\n",
        "    return train_transforms, val_test_transforms\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" DATA TRANSFORMATION and LOADERS \"\"\"\n",
        "\n",
        "### ===== For hyperparameter tuning considering train_data and val_data =====\n",
        "train_transforms, val_test_transforms = data_trasform(train_data)\n",
        "\n",
        "train_data.dataset.transform = train_transforms\n",
        "val_data.dataset.transform = val_test_transforms\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "val_loader   = DataLoader(val_data,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "\n",
        "\n",
        "### ===== For model testing considering tot_train_data and test_data =====\n",
        "train_transforms, val_test_transforms = data_trasform(tot_train_data)\n",
        "\n",
        "tot_train_data = torchvision.datasets.CIFAR100(root=ROOT, train=True, download=False, transform=train_transforms)\n",
        "test_data = torchvision.datasets.CIFAR100(root=ROOT, train=False, download=False, transform=val_test_transforms)\n",
        "\n",
        "tot_train_loader = DataLoader(tot_train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ddb98ed",
      "metadata": {
        "id": "8ddb98ed"
      },
      "source": [
        "## Functions Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "aeba326e",
      "metadata": {
        "id": "aeba326e"
      },
      "outputs": [],
      "source": [
        "\"\"\" SPLIT INTO K CLIENTS: iid and non-iid distributions  \"\"\"\n",
        "\n",
        "def iid_shard(dataset, K):\n",
        "    \"\"\"\n",
        "    Splits the dataset into K disjoint subsets (one per client) in an IID fashion. Each client receives ~len(dataset)/K samples,\n",
        "    drawn uniformly at random across all classes.\n",
        "    \"\"\"\n",
        "    num_items_per_client = len(dataset) // K\n",
        "    remainder = len(dataset) % K\n",
        "    all_indices = list(range(len(dataset)))\n",
        "    random.shuffle(all_indices)\n",
        "\n",
        "    client_data = {}  # Creation of a dictionary for clients\n",
        "    start = 0\n",
        "    for i in range(K):\n",
        "        # give +1 to the first `remainder` clients\n",
        "        add = num_items_per_client + (1 if i < remainder else 0)\n",
        "        end = start + add\n",
        "        client_data[i+1] = Subset(dataset, all_indices[start:end])\n",
        "        start = end\n",
        "\n",
        "    return client_data\n",
        "\n",
        "\n",
        "def noniid_shard(dataset, K, Nc):\n",
        "    \"\"\"\n",
        "    Splits the dataset into K disjoint subsets (one per client) in a non-IID fashion.\n",
        "    Disjoint dataset: each sample must belong to exactly one client and cannot be duplicated.\n",
        "    Nc: each client receives samples from Nc distinct classes, to simulate variability of data across clients.\n",
        "    \"\"\"\n",
        "    # === Feasibility check ===\n",
        "    all_classes = set(label for _, label in dataset)\n",
        "    num_classes = len(all_classes)\n",
        "    if Nc > num_classes:\n",
        "        raise ValueError(f\"Cannot assign {Nc} classes per client: only {num_classes} classes available.\")\n",
        "\n",
        "\n",
        "    # === Step 1: Organize data by class ===\n",
        "    # This ensures we can select samples from specific classes without duplication.\n",
        "    label_to_indices = defaultdict(list)\n",
        "    for idx, (_, label) in enumerate(dataset):\n",
        "        label_to_indices[label].append(idx)    # key: class, value: list of idxs of the corresponding class\n",
        "\n",
        "    # === Step 2: Shuffle samples within each class ===\n",
        "    # Prevents consecutive samples from going to the same clients repeatedly.\n",
        "    for cls in label_to_indices:\n",
        "        random.shuffle(label_to_indices[cls])\n",
        "\n",
        "    # === Step 3: Prepare the list of clients and shuffle it ===\n",
        "    # Guarantees random assignment of classes to clients.\n",
        "    all_classes = list(label_to_indices.keys())\n",
        "\n",
        "    client_class_map = {}\n",
        "    for client_id in range(1, K+1):        # Pre-select Nc classes per client at the beginning.\n",
        "        client_class_map[client_id] = random.sample(all_classes, Nc)\n",
        "\n",
        "    client_data = {cid: [] for cid in range(1, K+1)}    # Initialize empty client datasets\n",
        "    class_to_clients = defaultdict(list)     # For each class, keep track of clients that have it (reverse map from class → clients that have that class)\n",
        "    for cid, classes in client_class_map.items():\n",
        "        for cls in classes:\n",
        "            class_to_clients[cls].append(cid)\n",
        "\n",
        "\n",
        "    # === Step 4: Distribute samples in rounds ===\n",
        "    # Distributing samples to clients in rounds\n",
        "    for cls, indices in label_to_indices.items():    # Iterate over all classes, where indices is the list of the samples for the considered class\n",
        "        clients = class_to_clients[cls]  # clients that want this class\n",
        "        if not clients:\n",
        "            continue\n",
        "        i = 0\n",
        "        while indices:\n",
        "            client_id = clients[i % len(clients)]  # cycling continuously through the list of clients\n",
        "            sample = indices.pop()                 # take one sample and it removes it so it will not be repeated\n",
        "            client_data[client_id].append(sample)  # assign to this client\n",
        "            i += 1\n",
        "\n",
        "     # === Step 5: Convert sample index lists into Subsets ===\n",
        "    for cid in client_data:\n",
        "        client_data[cid] = Subset(dataset, client_data[cid])\n",
        "\n",
        "    return client_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34e82ddf",
      "metadata": {
        "id": "34e82ddf"
      },
      "source": [
        "## Class Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ff77908b",
      "metadata": {
        "id": "ff77908b"
      },
      "outputs": [],
      "source": [
        "def compress_mask(mask_dict):\n",
        "    \"\"\"\n",
        "    Convert each 0/1 tensor in the mask into a compressed bit array.\n",
        "    Returns a dictionary: {param_name: (packed_uint8_array, original_shape)}\n",
        "    \"\"\"\n",
        "    compressed = {}\n",
        "    for name, tensor in mask_dict.items():\n",
        "        # Move tensor to CPU, cast to uint8, flatten, and convert to numpy\n",
        "        arr = tensor.detach().to('cpu').to(torch.uint8).contiguous().view(-1).numpy()\n",
        "        # Pack bits into a compact representation (8x smaller than uint8)\n",
        "        packed = np.packbits(arr)\n",
        "        compressed[name] = (packed, tuple(tensor.shape))  # store packed data + original shape\n",
        "    return compressed\n",
        "\n",
        "\n",
        "def decompress_mask(compressed_dict):\n",
        "    \"\"\"\n",
        "    Reconstruct torch.uint8 tensors with the original shape (0/1 values).\n",
        "    \"\"\"\n",
        "    mask = {}\n",
        "    for name, (packed, shape) in compressed_dict.items():\n",
        "        total = int(np.prod(shape))  # number of elements in original tensor\n",
        "        # Unpack bits back to 0/1 and trim extra padding\n",
        "        unpacked = np.unpackbits(packed)[:total]\n",
        "        # Reshape to original tensor shape and convert to torch.uint8\n",
        "        mask[name] = torch.from_numpy(unpacked.reshape(shape)).to(torch.uint8)\n",
        "    return mask\n",
        "\n",
        "\"\"\" CLASS CLIENT\"\"\"\n",
        "class Client:\n",
        "    def __init__(self, client_id, dataset, loader,\n",
        "                 mask=None, extra_mask=None, fisher_scores=None,\n",
        "                 local_weights=None, local_fisher=None, compressed_mask=None):\n",
        "        self.client_id = client_id\n",
        "        self.dataset = dataset\n",
        "        self.loader = loader\n",
        "\n",
        "        # Full mask in RAM (0/1 tensors) – only needed during training\n",
        "        self.mask = mask\n",
        "\n",
        "        # Lightweight storage version (bit-packed mask)\n",
        "        self.compressed_mask = compressed_mask\n",
        "\n",
        "        # Optional: additional mask (depends on your logic)\n",
        "        self.extra_mask = extra_mask\n",
        "\n",
        "        # Fisher scores are only needed during calibration; cast to float16 if provided\n",
        "        self.fisher_scores = fisher_scores.half() if fisher_scores is not None else None\n",
        "\n",
        "        # Local weights (float16)\n",
        "        self.local_weights = local_weights.half() if local_weights is not None else None\n",
        "\n",
        "        # Local Fisher information (float16)\n",
        "        self.local_fisher = local_fisher.half() if local_fisher is not None else None\n",
        "\n",
        "    def compress_mask(self):\n",
        "        \"\"\"\n",
        "        Compress self.mask into self.compressed_mask and free RAM\n",
        "        \"\"\"\n",
        "        if self.mask is not None:\n",
        "            # Make sure mask tensors are uint8 (0/1 values)\n",
        "            self.mask = {k: v.to(torch.uint8) for k, v in self.mask.items()}\n",
        "            self.compressed_mask = compress_mask(self.mask)\n",
        "            self.mask = None  # release memory\n",
        "\n",
        "    def decompress_mask(self):\n",
        "        \"\"\"\n",
        "        Rebuild self.mask (0/1 tensors) from self.compressed_mask\n",
        "        \"\"\"\n",
        "        if self.compressed_mask is not None and self.mask is None:\n",
        "            self.mask = decompress_mask(self.compressed_mask)\n",
        "\n",
        "    def num_samples(self):\n",
        "        \"\"\"\n",
        "        Returns the number of samples in the client’s local dataset.\n",
        "        \"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    @staticmethod\n",
        "    def print_samples(clients):\n",
        "        \"\"\"\n",
        "        Prints the number of samples each client holds.\n",
        "        \"\"\"\n",
        "        print(\"Number of samples per client:\")\n",
        "        for client in clients:\n",
        "            print(f\"Client {client.client_id}: {client.num_samples()} samples\")\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_class_distribution(clients, dataset):\n",
        "        \"\"\"\n",
        "        Plots the distribution of classes for a list of clients.\n",
        "        \"\"\"\n",
        "        class_distributions = []\n",
        "\n",
        "        if hasattr(dataset, 'targets'):\n",
        "            get_label = lambda idx: dataset.targets[idx]\n",
        "        elif hasattr(dataset, 'labels'):\n",
        "            get_label = lambda idx: dataset.labels[idx]\n",
        "        else:\n",
        "            get_label = lambda idx: dataset[idx][1]\n",
        "\n",
        "        for client in clients:\n",
        "            indices = client.loader.dataset.indices if isinstance(client.loader.dataset, Subset) else list(range(len(client.loader.dataset)))\n",
        "            labels = [get_label(i) for i in indices]\n",
        "            class_counts = Counter(labels)\n",
        "            class_distributions.append(class_counts)\n",
        "\n",
        "        fig, axes = plt.subplots(nrows=1, ncols=len(clients), figsize=(5 * len(clients), 4))\n",
        "        if len(clients) == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for i, (client, class_counts) in enumerate(zip(clients, class_distributions)):\n",
        "            axes[i].bar(class_counts.keys(), class_counts.values(), color='orange')\n",
        "            axes[i].set_title(f'Client {client.client_id}')\n",
        "            axes[i].set_xlabel('Classes')\n",
        "            axes[i].set_ylabel('Frequency')\n",
        "            axes[i].set_xticks(list(class_counts.keys()))\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\"\"\" CLIENT CREATION \"\"\"\n",
        "def create_clients(data, K, Nc=None, flag=\"iid\", batch_size=64):\n",
        "    \"\"\"\n",
        "    Splits the dataset into K subsets, either IID or non-IID, and returns both the client datasets and their corresponding DataLoaders\n",
        "    \"\"\"\n",
        "    if flag == \"iid\":\n",
        "        client_datasets = iid_shard(data, K)  # Split the training data into K clients\n",
        "    else:\n",
        "        client_datasets = noniid_shard(data, K, Nc)\n",
        "\n",
        "    client_loaders = {}\n",
        "    for client_id, subset in client_datasets.items():\n",
        "        loader = DataLoader(subset, batch_size=batch_size, shuffle=True)\n",
        "        client_loaders[client_id] = loader\n",
        "    return client_datasets, client_loaders\n",
        "\n",
        "\n",
        "def create_client_objects(data, K, Nc=None, flag=\"iid\", batch_size=64, \n",
        "                          verbose=True, local_weights_dict=None, local_fisher_dict=None):\n",
        "    \"\"\"\n",
        "    Creates and returns a list of Client objects, each containing its own dataset and DataLoader,\n",
        "    with optional local_weights and local_fisher.\n",
        "    \"\"\"\n",
        "\n",
        "    if flag == \"non-iid\" and Nc is None:\n",
        "        raise ValueError(\"Nc must be set when flag='non-iid'\")\n",
        "\n",
        "    client_datasets, client_loaders = create_clients(data, K, Nc, flag=flag, batch_size=batch_size)\n",
        "    clients = []\n",
        "\n",
        "    for client_id in client_datasets.keys():\n",
        "        lw = local_weights_dict.get(client_id, None) if local_weights_dict else None\n",
        "        lf = local_fisher_dict.get(client_id, None) if local_fisher_dict else None\n",
        "\n",
        "        clients.append(Client(\n",
        "            client_id=client_id,\n",
        "            dataset=client_datasets[client_id],\n",
        "            loader=client_loaders[client_id],\n",
        "            local_weights=lw,\n",
        "            local_fisher=lf\n",
        "        ))\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Clients and their sample sizes:\")\n",
        "        for client in clients:\n",
        "            print(f\"Client {client.client_id}: {client.num_samples()} samples\")\n",
        "\n",
        "    return clients\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5007e58",
      "metadata": {
        "id": "e5007e58"
      },
      "source": [
        "## Server model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "adcf5d52",
      "metadata": {
        "id": "adcf5d52"
      },
      "outputs": [],
      "source": [
        "\"\"\" MODEL EVALUATION \"\"\"\n",
        "def evaluate_model(model, data_loader, criterion):\n",
        "    \"\"\"\n",
        "    The evaluate_model function computes the average loss and accuracy of a model on a dataset without updating its weights.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_corrects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            total_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader.dataset)\n",
        "    avg_acc = total_corrects.double() / len(data_loader.dataset)\n",
        "\n",
        "    return avg_loss, avg_acc.item()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "037d0b42",
      "metadata": {
        "id": "037d0b42"
      },
      "outputs": [],
      "source": [
        "\"\"\" CHECKPOINT FUNCTION \"\"\"\n",
        "def save_checkpoint(model, epoch, train_losses, train_accuracies,\n",
        "                    val_test_losses, val_test_accuracies, best_acc, best_loss, best_model_wts, path):\n",
        "    \"\"\"\n",
        "    The save_checkpoint function saves the model’s state, training/validation metrics,\n",
        "    and best performance to a specified file path.\n",
        "    \"\"\"\n",
        "    dir_name = os.path.dirname(path)\n",
        "    if dir_name:\n",
        "        os.makedirs(dir_name, exist_ok=True)\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'train_losses': train_losses,\n",
        "        'train_accuracies': train_accuracies,\n",
        "        'val_test_losses': val_test_losses,\n",
        "        'val_test_accuracies': val_test_accuracies,\n",
        "        'best_acc': best_acc,\n",
        "        'best_loss': best_loss,\n",
        "        'best_model_state_dict': best_model_wts\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "\n",
        "\n",
        "\n",
        "def init_checkpoint(model, path=None, device='cpu', verbose = True):\n",
        "    \"\"\"\n",
        "    Initialize a checkpoint. If path is None, create default checkpoint with empty/default values.\n",
        "    If path is given and file exists, load it.\n",
        "    Returns: start_epoch, best_acc, best_loss, train_losses, train_accuracies, val_test_losses, val_test_accuracies, checkpoint_path, best_model_wts\n",
        "    \"\"\"\n",
        "    if path is None:\n",
        "        # default path\n",
        "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "        path = \"checkpoints/latest.pth\"\n",
        "        if verbose:\n",
        "            print(f\"Initializing new checkpoint at {path}\")\n",
        "        # save default empty checkpoint\n",
        "        checkpoint = {\n",
        "            'epoch': 1,\n",
        "            'best_acc': 0.0,\n",
        "            'best_loss': 1e10,\n",
        "            'train_losses': [],\n",
        "            'train_accuracies': [],\n",
        "            'val_test_losses': [],\n",
        "            'val_test_accuracies': [],\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'best_model_state_dict': copy.deepcopy(model.state_dict())\n",
        "        }\n",
        "        torch.save(checkpoint, path)\n",
        "        return 1, 0.0, 1e10, [], [], [], [], path, copy.deepcopy(model.state_dict())\n",
        "\n",
        "    else:\n",
        "        # load existing checkpoint\n",
        "        if not os.path.isfile(path):\n",
        "            raise FileNotFoundError(f\"Checkpoint file {path} does not exist.\")\n",
        "\n",
        "        print(f\"Loading checkpoint from {path}\")\n",
        "        checkpoint = torch.load(path, map_location=device, weights_only=False)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        best_model_wts = checkpoint.get('best_model_state_dict', copy.deepcopy(model.state_dict()))\n",
        "        return (checkpoint['epoch'],\n",
        "                checkpoint.get('best_acc', 0.0),\n",
        "                checkpoint.get('best_loss', 1e10),\n",
        "                checkpoint.get('train_losses', []),\n",
        "                checkpoint.get('train_accuracies', []),\n",
        "                checkpoint.get('val_test_losses', []),\n",
        "                checkpoint.get('val_test_accuracies', []),\n",
        "                path,\n",
        "                best_model_wts )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17af90e4",
      "metadata": {
        "id": "17af90e4"
      },
      "source": [
        "## Train client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "a0823dc7",
      "metadata": {
        "id": "a0823dc7"
      },
      "outputs": [],
      "source": [
        "\"\"\" TRAINING FUNCTION \"\"\"\n",
        "def train_model_client(model, criterion, optimizer, scheduler, client: Client, J=5, device=None):    ### Single client\n",
        "    \"\"\"\n",
        "    Trains a model locally on a single client for J epochs.\n",
        "    Performs forward pass, computes loss, backpropagation, and updates weights using the given optimizer.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    train_loader = client.loader\n",
        "\n",
        "    # ========== TRAINING ==========\n",
        "    for j in range(J):   # \"inner\" loop\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            train_corrects += torch.sum(preds == labels).item()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        epoch_train_loss = train_loss / len(train_loader.dataset)\n",
        "        epoch_train_acc = train_corrects / len(train_loader.dataset)\n",
        "\n",
        "    return epoch_train_loss, epoch_train_acc    # Return the last performances\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e886bc0",
      "metadata": {
        "id": "8e886bc0"
      },
      "source": [
        "## Mask & Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c93c222f",
      "metadata": {
        "id": "c93c222f"
      },
      "outputs": [],
      "source": [
        "\"\"\" MASK COMPUTATION & CALIBRATION \"\"\"\n",
        "\n",
        "def compute_fisher_scores(client, model, device):\n",
        "    \"\"\"\n",
        "    Compute diagonal Fisher Information scores and store them in client.fisher_scores.\n",
        "    Uses client.loader as the dataloader.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize Fisher scores on the client if not already present\n",
        "    if not hasattr(client, 'fisher_scores') or client.fisher_scores is None:\n",
        "        client.fisher_scores = {name: torch.zeros_like(param, device=\"cpu\")\n",
        "                                for name, param in model.named_parameters() if param.requires_grad}\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(client.loader):\n",
        "        #if i > 5:  # max 5 batches\n",
        "        #    break\n",
        "\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # log-probabilities\n",
        "        log_probs = F.log_softmax(outputs, dim=1)\n",
        "\n",
        "        # Sample y ~ p(y|x)\n",
        "        sampled_y = torch.multinomial(log_probs.exp(), num_samples=1).squeeze(-1)\n",
        "\n",
        "        # NLL loss on sampled labels\n",
        "        loss = F.nll_loss(log_probs, sampled_y)\n",
        "        loss.backward()\n",
        "\n",
        "        # Accumulate squared gradients\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                client.fisher_scores[name] += (param.grad.detach().cpu() ** 2)\n",
        "\n",
        "    # Normalize\n",
        "    for name in client.fisher_scores.keys():\n",
        "        client.fisher_scores[name] /= len(client.loader.dataset)\n",
        "\n",
        "\n",
        "def calibrate_mask_for_client(client, model, device, R=5, final_sparsity=0.9, lr=0.005, weight_decay=0.00005 , keep=\"least\"):\n",
        "    # Initial mask: all ones (uint8) only for trainable parameters\n",
        "    mask = {name: torch.ones_like(param, device=\"cpu\", dtype=torch.uint8)\n",
        "            for name, param in model.named_parameters() if param.requires_grad}\n",
        "    client.mask = mask  # Stored in RAM\n",
        "\n",
        "    # Recompute Fisher and update the mask R times\n",
        "    for r in range(1, R + 1):\n",
        "        current_density = (1 - final_sparsity) ** (r / R)\n",
        "        current_sparsity = 1 - current_density\n",
        "        print(f\"[Round = {r}] Sparsity = {current_sparsity:.4f}\")\n",
        "\n",
        "        compute_fisher_scores(client, model, device)\n",
        "        fisher_scores = client.fisher_scores  # dict {name: tensor float}\n",
        "\n",
        "        # --- Flatten all scores ---\n",
        "        all_scores = torch.cat([score.view(-1) for score in fisher_scores.values()])\n",
        "\n",
        "        # --- Compute global threshold using topk ---\n",
        "        num_keep_global = int(len(all_scores) * current_density)\n",
        "        num_keep_global = max(1, min(num_keep_global, len(all_scores)))\n",
        "\n",
        "        if keep == \"least\":\n",
        "            # keep the least important weights\n",
        "            _, idx = torch.topk(all_scores, k=num_keep_global, largest=False)\n",
        "        elif keep == \"most\":\n",
        "            # keep the most important weights\n",
        "            _, idx = torch.topk(all_scores, k=num_keep_global, largest=True)\n",
        "        elif keep == \"random\":\n",
        "            idx = torch.randperm(len(all_scores))[:num_keep_global]\n",
        "        else:\n",
        "            raise ValueError(\"keep must be 'least', 'most', or 'random'\")\n",
        "\n",
        "        # build global_keep\n",
        "        global_keep = torch.zeros_like(all_scores, dtype=torch.bool)\n",
        "        global_keep[idx] = True\n",
        "\n",
        "        # --- Redistribute threshold layer by layer ---\n",
        "        new_mask = {}\n",
        "        start = 0\n",
        "        for name, score in fisher_scores.items():\n",
        "            numel = score.numel()\n",
        "            keep_tensor = global_keep[start:start+numel].view_as(score)\n",
        "            new_mask[name] = (keep_tensor.to(torch.uint8) * client.mask[name])\n",
        "            start += numel\n",
        "\n",
        "        # update current mask\n",
        "        client.mask = new_mask\n",
        "        total_ones = sum(mask.sum().item() for mask in client.mask.values())\n",
        "        total_params = sum(mask.numel() for mask in client.mask.values())\n",
        "        perc_active = 100 * total_ones / total_params\n",
        "        print(f\"Active parameters: {total_ones}/{total_params} ({perc_active:.2f}%)\")\n",
        "\n",
        "        # mini-training with SparseSGD + current mask\n",
        "        param_to_name = {id(param): n for n, param in model.named_parameters()}\n",
        "        optimizer = SparseSGD(model.parameters(), lr=lr, weight_decay=weight_decay, mask_dict=client.mask, param_to_name=param_to_name)\n",
        "        local_model = copy.deepcopy(model).to(device)\n",
        "        train_model_client(local_model, criterion=nn.CrossEntropyLoss(), optimizer=optimizer,\n",
        "                           scheduler=None, client=client, J=1, device=device)\n",
        "\n",
        "    # End of calibration: do not keep Fisher scores in memory\n",
        "    client.fisher_scores = None\n",
        "\n",
        "    # Compress mask for lighter storage\n",
        "    client.compress_mask()\n",
        "\n",
        "\n",
        "def calibrate_all_clients(clients, global_model, device, R=5, final_sparsity=0.9, lr=0.005, weight_decay=0.00005, keep =\"least\"):\n",
        "    \"\"\"\n",
        "    Apply mask calibration for all clients.\n",
        "    \"\"\"\n",
        "    for client in clients:\n",
        "        calibrate_mask_for_client(client, global_model, device, R=R, final_sparsity=final_sparsity, lr=lr, weight_decay=weight_decay, keep=keep)\n",
        "        print(f\"Client {client.client_id} -> mask calibrated (sparsity={final_sparsity})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9fdaad5",
      "metadata": {
        "id": "e9fdaad5"
      },
      "source": [
        "## SparseSGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e79dce9",
      "metadata": {
        "id": "0e79dce9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import SGD\n",
        "\n",
        "class SparseSGD(torch.optim.SGD):\n",
        "    def __init__(self, params, lr=0.01, momentum=0, weight_decay=0, mask_dict=None, param_to_name=None):\n",
        "        super().__init__(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "        self.mask_dict = mask_dict  # {name: mask_tensor su CPU}\n",
        "        self.param_to_name = param_to_name or {}\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        " \n",
        "        if self.mask_dict is not None:\n",
        "            for group in self.param_groups:\n",
        "                for p in group['params']:\n",
        "                    if p.grad is None:\n",
        "                        continue\n",
        "                    name = self.param_to_name.get(id(p), None)\n",
        "                    if name is not None and name in self.mask_dict:\n",
        "                        mask = self.mask_dict[name].to(p.device)\n",
        "                        p.grad.mul_(mask.to(dtype=p.grad.dtype))     # CORRESPONDS TO: d_p = d_p * mask\n",
        "\n",
        "        super().step(closure)\n",
        "\n",
        "        return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6cb5b9d",
      "metadata": {
        "id": "d6cb5b9d"
      },
      "source": [
        "## Main Training FL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cf7a085",
      "metadata": {},
      "outputs": [],
      "source": [
        "def init_local_model_with_fisher(global_model, global_fisher_scores, client, criterion, device, J=1):\n",
        "    \"\"\"\n",
        "    Initialize a local model starting from the global model, using the client's local weights\n",
        "    and a coefficient based on the ratio client_Fisher_score / global_Fisher_score.\n",
        "\n",
        "    Args:\n",
        "        global_model: updated global model (PyTorch nn.Module)\n",
        "        global_fisher_scores: dict {param_name: fisher_score (float16/float32)}\n",
        "        client: Client object with client.local_weights already populated (float16)\n",
        "        criterion: loss function for client Fisher score calculation\n",
        "        device: device for the model\n",
        "        J: number of batches for client Fisher score calculation (optional)\n",
        "\n",
        "    Returns:\n",
        "        temp_model: PyTorch local model with weights mixed according to Fisher score\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    temp_model = copy.deepcopy(global_model).to(device)\n",
        "    temp_model.eval()  # evaluation mode for Fisher score calculation\n",
        "\n",
        "    # --- TEMPORARY CLIENT FISHER SCORE CALCULATION ---\n",
        "    client_fisher = {name: torch.zeros_like(param, device=\"cpu\")\n",
        "                     for name, param in temp_model.named_parameters() if param.requires_grad}\n",
        "\n",
        "    # Use only J batches for faster computation\n",
        "    loader_iter = iter(client.loader)\n",
        "    for i in range(J):\n",
        "        try:\n",
        "            inputs, labels = next(loader_iter)\n",
        "        except StopIteration:\n",
        "            break\n",
        "\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        temp_model.zero_grad()\n",
        "        outputs = temp_model(inputs)\n",
        "        log_probs = F.log_softmax(outputs, dim=1)\n",
        "        sampled_y = torch.multinomial(log_probs.exp(), num_samples=1).squeeze(-1)\n",
        "        loss = F.nll_loss(log_probs, sampled_y)\n",
        "        loss.backward()\n",
        "\n",
        "        for name, param in temp_model.named_parameters():\n",
        "            if param.requires_grad and param.grad is not None:\n",
        "                client_fisher[name] += (param.grad.detach().cpu() ** 2)\n",
        "\n",
        "    # Normalize by the number of client examples\n",
        "    for name in client_fisher.keys():\n",
        "        client_fisher[name] /= len(client.loader.dataset)\n",
        "\n",
        "    # --- BUILD NEW WEIGHTS ---\n",
        "    new_state_dict = {}\n",
        "    global_state_dict = {k: v.cpu() for k, v in global_model.state_dict().items()}\n",
        "\n",
        "    for name, param in temp_model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            fisher_client_val = client_fisher[name].to(torch.float16)\n",
        "            fisher_global_val = global_fisher_scores[name].to(torch.float16)\n",
        "\n",
        "            # Avoid division by zero\n",
        "            coeff = torch.where(fisher_global_val != 0,\n",
        "                                fisher_client_val / fisher_global_val,\n",
        "                                torch.zeros_like(fisher_client_val))\n",
        "\n",
        "            # Clip coeff between 0 and 1 for stability\n",
        "            coeff = torch.clamp(coeff, 0.0, 1.0)\n",
        "\n",
        "            # Combine weights: coeff * local_weights + (1 - coeff) * global_weights\n",
        "            lw = client.local_weights[name].to(torch.float16)\n",
        "            gw = global_state_dict[name].to(torch.float16)\n",
        "            new_param = coeff * lw + (1.0 - coeff) * gw\n",
        "            new_state_dict[name] = new_param\n",
        "        else:\n",
        "            # Frozen parameters (e.g., head) remain global\n",
        "            new_state_dict[name] = global_state_dict[name].to(torch.float16)\n",
        "\n",
        "    # Update temp_model with new weights\n",
        "    temp_model.load_state_dict(new_state_dict)\n",
        "\n",
        "    # --- CLEANUP ---\n",
        "    del client_fisher\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return temp_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab00b0eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_test_model_FL(global_model, criterion, LR, MOMENTUM, WEIGHT_DECAY, T_MAX, clients, val_test_loader,\n",
        "                        num_epochs, C, J, checkpoint_path=None, checkpoints=True, verbose=True, use_sparse=False, global_fisher_scores = []):\n",
        "    \"\"\"\n",
        "    Federated Learning training loop with FedAvg aggregation and optional SparseSGD.\n",
        "    Stores local weights in client.local_weights (float16) after local training.\n",
        "    Uses Fisher-weighted initialization from the second round onward.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    val_test_loss_s, val_test_acc_s, avg_train_loss_s, avg_train_acc_s = [], [], [], []\n",
        "\n",
        "    # Initial copy of global weights\n",
        "    fed_model_weights = {k: v.clone().detach() for k, v in global_model.state_dict().items()}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        round_train_losses = []\n",
        "        round_train_accs = []\n",
        "\n",
        "        if verbose or epoch == 0 or (epoch + 1) % 5 == 0:\n",
        "            print(f\"\\n--- Federated Round {epoch+1}/{num_epochs} ---\")\n",
        "\n",
        "        # Sample a fraction of clients\n",
        "        num_clients = max(1, int(C * len(clients)))\n",
        "        selected_clients = np.random.choice(clients, num_clients, replace=False)\n",
        "\n",
        "        local_weights = []\n",
        "        local_sizes = []\n",
        "\n",
        "        for client in selected_clients:\n",
        "            # --- LOCAL MODEL INITIALIZATION ---\n",
        "            if client.local_weights is None:\n",
        "                # First round: standard copy of global model\n",
        "                local_model = copy.deepcopy(global_model)\n",
        "                local_model.load_state_dict(fed_model_weights)\n",
        "                local_model.to(device)\n",
        "            else:\n",
        "                # From the second round: use Fisher-weighted initialization\n",
        "                local_model = init_local_model_with_fisher(global_model, global_fisher_scores, client, criterion, device, J=J)\n",
        "                local_model.to(device)\n",
        "\n",
        "            # Build mapping param->name\n",
        "            param_to_name = {id(param): name for name, param in local_model.named_parameters()}\n",
        "\n",
        "            # --- OPTIMIZER ---\n",
        "            if use_sparse:\n",
        "                client.decompress_mask()\n",
        "                optimizer = SparseSGD(local_model.parameters(),\n",
        "                                      lr=LR,\n",
        "                                      momentum=MOMENTUM,\n",
        "                                      weight_decay=WEIGHT_DECAY,\n",
        "                                      mask_dict=client.mask,\n",
        "                                      param_to_name=param_to_name)\n",
        "            else:\n",
        "                optimizer = torch.optim.SGD(local_model.parameters(),\n",
        "                                            lr=LR,\n",
        "                                            momentum=MOMENTUM,\n",
        "                                            weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_MAX) if T_MAX else None\n",
        "\n",
        "            # --- TRAIN CLIENT ---\n",
        "            train_loss, train_acc = train_model_client(local_model, criterion, optimizer, scheduler, client, J, device=device)\n",
        "            round_train_losses.append(train_loss)\n",
        "            round_train_accs.append(train_acc)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Client {client.client_id} -> Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "            # --- SAVE LOCAL WEIGHTS FOR FEDAVG AND CLIENT ---\n",
        "            state_dict_cpu = {k: v.cpu().detach() for k, v in local_model.state_dict().items()}\n",
        "            local_weights.append(state_dict_cpu)\n",
        "            local_sizes.append(client.num_samples())\n",
        "\n",
        "            # Save in float16 to reduce memory usage\n",
        "            client.local_weights = {k: v.half() for k, v in state_dict_cpu.items()}\n",
        "\n",
        "            # Free GPU memory\n",
        "            del local_model, optimizer, scheduler\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # --- FEDAVG ---\n",
        "        total_samples = sum(local_sizes)\n",
        "        fed_model_weights = {}\n",
        "        for key in local_weights[0].keys():\n",
        "            fed_model_weights[key] = sum([local_weights[i][key] * (local_sizes[i]/total_samples)\n",
        "                                          for i in range(len(local_weights))])\n",
        "\n",
        "        global_model.load_state_dict(fed_model_weights)\n",
        "\n",
        "        # --- GLOBAL EVALUATION ---\n",
        "        avg_train_loss = np.mean(round_train_losses)\n",
        "        avg_train_acc = np.mean(round_train_accs)\n",
        "        val_test_loss, val_test_acc = evaluate_model(global_model, val_test_loader, criterion)\n",
        "        if verbose or epoch == 0 or (epoch + 1) % 5 == 0:\n",
        "            print(f\"Round {epoch+1} -> Avg Train Loss: {avg_train_loss:.4f}, Avg Train Acc: {avg_train_acc:.4f}\")\n",
        "            print(f\"Round {epoch+1} -> Val Loss: {val_test_loss:.4f}, Val Acc: {val_test_acc:.4f}\")\n",
        "\n",
        "        val_test_loss_s.append(val_test_loss)\n",
        "        val_test_acc_s.append(val_test_acc)\n",
        "        avg_train_loss_s.append(avg_train_loss)\n",
        "        avg_train_acc_s.append(avg_train_acc)\n",
        "\n",
        "        # --- CHECKPOINT ---\n",
        "        if checkpoints and checkpoint_path:\n",
        "            torch.save(global_model.state_dict(), f\"{checkpoint_path}_round{epoch+1}.pth\")\n",
        "\n",
        "    return global_model, val_test_loss_s, val_test_acc_s, avg_train_loss_s, avg_train_acc_s\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e889421d",
      "metadata": {
        "id": "e889421d"
      },
      "source": [
        "# _________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac779ce4",
      "metadata": {
        "id": "ac779ce4"
      },
      "outputs": [],
      "source": [
        "set_seeds(123)\n",
        "checkpoint_path = None\n",
        "\n",
        "\n",
        "###########################\n",
        "# === PARAMETERS TO SET ===\n",
        "FLAG = \"niid\"    # 'niid'\n",
        "Nc = 5       # {1,5,10,50}\n",
        "J = 4           # {4,8,16}\n",
        "SPARSITY = 0.5\n",
        "C_ROUNDS = 2     # calibration rounds\n",
        "############################\n",
        "\n",
        "\n",
        "# === GENERAL PARAMETERS ===\n",
        "N_EP = 50\n",
        "#K = 50   \n",
        "C = 0.1\n",
        "LR = 1e-4\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 5e-5\n",
        "T_MAX = J\n",
        "VERBOSE = True\n",
        "KEEP = 'least'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "e0a904ed",
      "metadata": {
        "id": "e0a904ed",
        "outputId": "45ef51e6-4056-4d17-87ee-c9aebd3936d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /home/gabriele/.cache/torch/hub/facebookresearch_dino_main\n",
            "/tmp/ipykernel_341415/2574615723.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  global_model_cpu.load_state_dict(torch.load(model_filename, map_location=device_cpu))\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2500x400 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\"\"\" IMPORTING THE MODEL FROM OUTSIDE \"\"\"\n",
        "\n",
        "# === CREATE and LOAD MODEL on CPU ===\n",
        "device_cpu = torch.device(\"cpu\")\n",
        "dino_vits16 = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
        "dino_vits16 = dino_vits16.to(device_cpu)   # resta su CPU\n",
        "\n",
        "global_model_cpu = DINOWithHead(backbone=dino_vits16, num_classes=100).to(device_cpu)\n",
        "model_filename = f\"global_model_FL_HEAD_{FLAG}_J{J}_Nc{Nc}_50.pth\"\n",
        "global_model_cpu.load_state_dict(torch.load(model_filename, map_location=device_cpu))\n",
        "global_model_cpu.to(device_cpu)\n",
        "global_model_cpu.train()\n",
        "\n",
        "# === FREEZE SOLO LA HEAD, BACKBONE TRAINABLE ===\n",
        "for name, param in global_model_cpu.named_parameters():\n",
        "    if name.startswith(\"head\"):\n",
        "        param.requires_grad = False\n",
        "    else:\n",
        "        param.requires_grad = True\n",
        "\n",
        "\n",
        "# === CREA LE DUE COPIE SU GPU ===\n",
        "device_gpu = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "#imported_model_calib = copy.deepcopy(global_model_cpu).to(device_gpu)\n",
        "imported_model_train = copy.deepcopy(global_model_cpu).to(device_gpu)\n",
        "\n",
        "\n",
        "# === IMPORTING CLIENTS ==\n",
        "clients_filename = f\"clients_FL_HEAD_{FLAG}_J{J}_Nc{Nc}_50.pkl\"\n",
        "with open(clients_filename, 'rb') as f:\n",
        "    clients = pickle.load(f)\n",
        "\n",
        "Client.plot_class_distribution(clients[0:5], tot_train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "3b3d04d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_global_fisher(clients, model, device):\n",
        "    \"\"\"\n",
        "    Compute cumulative (global) Fisher scores over all clients without storing\n",
        "    individual client Fisher scores. Returns a dictionary {param_name: fisher_score}.\n",
        "    \n",
        "    Args:\n",
        "        clients: list of Client objects\n",
        "        model: the PyTorch model\n",
        "        device: 'cpu' or 'cuda'\n",
        "    \"\"\"\n",
        "    # Initialize global Fisher scores only for trainable parameters\n",
        "    global_fisher = {name: torch.zeros_like(param, device=\"cpu\")\n",
        "                     for name, param in model.named_parameters() if param.requires_grad}\n",
        "\n",
        "    for client in clients:\n",
        "        model.eval()\n",
        "        for inputs, labels in client.loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            model.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            log_probs = F.log_softmax(outputs, dim=1)\n",
        "            sampled_y = torch.multinomial(log_probs.exp(), num_samples=1).squeeze(-1)\n",
        "            loss = F.nll_loss(log_probs, sampled_y)\n",
        "            loss.backward()\n",
        "\n",
        "            # Accumulate squared gradients into global Fisher scores\n",
        "            for name, param in model.named_parameters():\n",
        "                if param.requires_grad and param.grad is not None:\n",
        "                    global_fisher[name] += (param.grad.detach().cpu() ** 2) / len(client.loader.dataset)\n",
        "\n",
        "    return global_fisher\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e36a720-63b1-46dc-abee-14ef5b187714",
      "metadata": {
        "id": "8e36a720-63b1-46dc-abee-14ef5b187714",
        "outputId": "40632a2d-d353-4822-ff5d-7e728a924916"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Masking, selection method: least\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10698327/21665664 (49.38%)\n",
            "✅ Client 1 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10687349/21665664 (49.33%)\n",
            "✅ Client 2 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10760903/21665664 (49.67%)\n",
            "✅ Client 3 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10716555/21665664 (49.46%)\n",
            "✅ Client 4 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10740325/21665664 (49.57%)\n",
            "✅ Client 5 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10694741/21665664 (49.36%)\n",
            "✅ Client 6 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10710704/21665664 (49.44%)\n",
            "✅ Client 7 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10785151/21665664 (49.78%)\n",
            "✅ Client 8 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10684685/21665664 (49.32%)\n",
            "✅ Client 9 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10722680/21665664 (49.49%)\n",
            "✅ Client 10 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10754109/21665664 (49.64%)\n",
            "✅ Client 11 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10728686/21665664 (49.52%)\n",
            "✅ Client 12 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10764864/21665664 (49.69%)\n",
            "✅ Client 13 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10696396/21665664 (49.37%)\n",
            "✅ Client 14 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10743299/21665664 (49.59%)\n",
            "✅ Client 15 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10776823/21665664 (49.74%)\n",
            "✅ Client 16 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10698235/21665664 (49.38%)\n",
            "✅ Client 17 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10694492/21665664 (49.36%)\n",
            "✅ Client 18 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10757620/21665664 (49.65%)\n",
            "✅ Client 19 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10707642/21665664 (49.42%)\n",
            "✅ Client 20 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10721689/21665664 (49.49%)\n",
            "✅ Client 21 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10731422/21665664 (49.53%)\n",
            "✅ Client 22 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10728272/21665664 (49.52%)\n",
            "✅ Client 23 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10738527/21665664 (49.56%)\n",
            "✅ Client 24 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10385638/21665664 (47.94%)\n",
            "✅ Client 25 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10762522/21665664 (49.68%)\n",
            "✅ Client 26 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10729850/21665664 (49.52%)\n",
            "✅ Client 27 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10768398/21665664 (49.70%)\n",
            "✅ Client 28 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10736661/21665664 (49.56%)\n",
            "✅ Client 29 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10660383/21665664 (49.20%)\n",
            "✅ Client 30 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10742926/21665664 (49.59%)\n",
            "✅ Client 31 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10710795/21665664 (49.44%)\n",
            "✅ Client 32 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10613290/21665664 (48.99%)\n",
            "✅ Client 33 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10761594/21665664 (49.67%)\n",
            "✅ Client 34 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10719324/21665664 (49.48%)\n",
            "✅ Client 35 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10743121/21665664 (49.59%)\n",
            "✅ Client 36 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10802863/21665664 (49.86%)\n",
            "✅ Client 37 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10812732/21665664 (49.91%)\n",
            "✅ Client 38 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10778982/21665664 (49.75%)\n",
            "✅ Client 39 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10750533/21665664 (49.62%)\n",
            "✅ Client 40 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10779883/21665664 (49.76%)\n",
            "✅ Client 41 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10686814/21665664 (49.33%)\n",
            "✅ Client 42 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10742533/21665664 (49.58%)\n",
            "✅ Client 43 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10777209/21665664 (49.74%)\n",
            "✅ Client 44 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10634002/21665664 (49.08%)\n",
            "✅ Client 45 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10722198/21665664 (49.49%)\n",
            "✅ Client 46 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10596213/21665664 (48.91%)\n",
            "✅ Client 47 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10739393/21665664 (49.57%)\n",
            "✅ Client 48 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10767181/21665664 (49.70%)\n",
            "✅ Client 49 -> mask calibrated (sparsity=0.5)\n",
            "[Round = 1] Sparsity = 0.2929\n",
            "Parametri attivi: 15319937/21665664 (70.71%)\n",
            "[Round = 2] Sparsity = 0.5000\n",
            "Parametri attivi: 10698572/21665664 (49.38%)\n",
            "✅ Client 50 -> mask calibrated (sparsity=0.5)\n",
            "Calcolating global Fisher scores before FL training...\n",
            "\n",
            "--- Federated Round 1/50 ---\n",
            "Client 11 -> Train Loss: 0.0897, Train Acc: 0.9794\n",
            "Client 14 -> Train Loss: 0.0252, Train Acc: 0.9963\n",
            "Client 31 -> Train Loss: 0.0623, Train Acc: 0.9863\n",
            "Client 47 -> Train Loss: 0.0899, Train Acc: 0.9788\n",
            "Client 19 -> Train Loss: 0.0355, Train Acc: 0.9920\n",
            "Round 1 -> Avg Train Loss: 0.0605, Avg Train Acc: 0.9866\n",
            "Round 1 -> Val Loss: 3.8624, Val Acc: 0.5040\n",
            "\n",
            "--- Federated Round 2/50 ---\n",
            "Client 5 -> Train Loss: 0.0778, Train Acc: 0.9809\n",
            "Client 6 -> Train Loss: 0.0460, Train Acc: 0.9924\n",
            "Client 1 -> Train Loss: 0.0429, Train Acc: 0.9909\n",
            "Client 38 -> Train Loss: 0.0101, Train Acc: 0.9981\n",
            "Client 24 -> Train Loss: 0.0249, Train Acc: 0.9933\n",
            "Round 2 -> Avg Train Loss: 0.0404, Avg Train Acc: 0.9911\n",
            "Round 2 -> Val Loss: 3.4177, Val Acc: 0.5386\n",
            "\n",
            "--- Federated Round 3/50 ---\n",
            "Client 41 -> Train Loss: 0.0562, Train Acc: 0.9877\n",
            "Client 48 -> Train Loss: 0.0482, Train Acc: 0.9874\n",
            "Client 3 -> Train Loss: 0.0302, Train Acc: 0.9929\n",
            "Client 25 -> Train Loss: 0.0390, Train Acc: 0.9855\n",
            "Client 19 -> Train Loss: 0.0257, Train Acc: 0.9947\n",
            "Round 3 -> Avg Train Loss: 0.0399, Avg Train Acc: 0.9896\n",
            "Round 3 -> Val Loss: 2.9940, Val Acc: 0.5688\n",
            "\n",
            "--- Federated Round 4/50 ---\n",
            "Client 44 -> Train Loss: 0.0220, Train Acc: 0.9943\n",
            "Client 5 -> Train Loss: 0.0516, Train Acc: 0.9899\n",
            "Client 1 -> Train Loss: 0.0292, Train Acc: 0.9948\n",
            "Client 33 -> Train Loss: 0.0272, Train Acc: 0.9940\n",
            "Client 50 -> Train Loss: 0.0385, Train Acc: 0.9898\n",
            "Round 4 -> Avg Train Loss: 0.0337, Avg Train Acc: 0.9926\n",
            "Round 4 -> Val Loss: 2.9298, Val Acc: 0.5733\n",
            "\n",
            "--- Federated Round 5/50 ---\n",
            "Client 23 -> Train Loss: 0.0187, Train Acc: 0.9966\n",
            "Client 21 -> Train Loss: 0.0139, Train Acc: 0.9981\n",
            "Client 40 -> Train Loss: 0.0932, Train Acc: 0.9708\n",
            "Client 26 -> Train Loss: 0.0503, Train Acc: 0.9815\n",
            "Client 47 -> Train Loss: 0.0567, Train Acc: 0.9859\n",
            "Round 5 -> Avg Train Loss: 0.0466, Avg Train Acc: 0.9866\n",
            "Round 5 -> Val Loss: 2.8782, Val Acc: 0.5795\n",
            "\n",
            "--- Federated Round 6/50 ---\n",
            "Client 43 -> Train Loss: 0.0606, Train Acc: 0.9897\n",
            "Client 32 -> Train Loss: 0.0909, Train Acc: 0.9773\n",
            "Client 14 -> Train Loss: 0.0230, Train Acc: 0.9963\n",
            "Client 23 -> Train Loss: 0.0139, Train Acc: 1.0000\n",
            "Client 30 -> Train Loss: 0.0893, Train Acc: 0.9738\n",
            "Round 6 -> Avg Train Loss: 0.0556, Avg Train Acc: 0.9874\n",
            "Round 6 -> Val Loss: 2.8914, Val Acc: 0.5747\n",
            "\n",
            "--- Federated Round 7/50 ---\n",
            "Client 19 -> Train Loss: 0.0139, Train Acc: 0.9982\n",
            "Client 12 -> Train Loss: 0.0385, Train Acc: 0.9901\n",
            "Client 24 -> Train Loss: 0.0171, Train Acc: 0.9971\n",
            "Client 9 -> Train Loss: 0.0299, Train Acc: 0.9919\n",
            "Client 25 -> Train Loss: 0.0181, Train Acc: 0.9969\n",
            "Round 7 -> Avg Train Loss: 0.0235, Avg Train Acc: 0.9949\n",
            "Round 7 -> Val Loss: 2.7900, Val Acc: 0.5801\n",
            "\n",
            "--- Federated Round 8/50 ---\n",
            "Client 12 -> Train Loss: 0.0311, Train Acc: 0.9951\n",
            "Client 44 -> Train Loss: 0.0128, Train Acc: 0.9981\n",
            "Client 5 -> Train Loss: 0.0358, Train Acc: 0.9933\n",
            "Client 18 -> Train Loss: 0.0300, Train Acc: 0.9938\n",
            "Client 47 -> Train Loss: 0.0396, Train Acc: 0.9901\n",
            "Round 8 -> Avg Train Loss: 0.0299, Avg Train Acc: 0.9941\n",
            "Round 8 -> Val Loss: 2.7689, Val Acc: 0.5812\n",
            "\n",
            "--- Federated Round 9/50 ---\n",
            "Client 5 -> Train Loss: 0.0295, Train Acc: 0.9955\n",
            "Client 34 -> Train Loss: 0.0219, Train Acc: 0.9941\n",
            "Client 40 -> Train Loss: 0.0715, Train Acc: 0.9823\n",
            "Client 45 -> Train Loss: 0.0341, Train Acc: 0.9915\n",
            "Client 12 -> Train Loss: 0.0273, Train Acc: 0.9938\n",
            "Round 9 -> Avg Train Loss: 0.0369, Avg Train Acc: 0.9914\n",
            "Round 9 -> Val Loss: 2.7319, Val Acc: 0.5802\n",
            "\n",
            "--- Federated Round 10/50 ---\n",
            "Client 23 -> Train Loss: 0.0098, Train Acc: 1.0000\n",
            "Client 45 -> Train Loss: 0.0269, Train Acc: 0.9932\n",
            "Client 6 -> Train Loss: 0.0282, Train Acc: 0.9967\n",
            "Client 28 -> Train Loss: 0.0577, Train Acc: 0.9852\n",
            "Client 44 -> Train Loss: 0.0086, Train Acc: 0.9991\n",
            "Round 10 -> Avg Train Loss: 0.0263, Avg Train Acc: 0.9948\n",
            "Round 10 -> Val Loss: 2.7010, Val Acc: 0.5851\n",
            "\n",
            "--- Federated Round 11/50 ---\n",
            "Client 23 -> Train Loss: 0.0090, Train Acc: 1.0000\n",
            "Client 22 -> Train Loss: 0.0359, Train Acc: 0.9938\n",
            "Client 3 -> Train Loss: 0.0180, Train Acc: 0.9960\n",
            "Client 12 -> Train Loss: 0.0211, Train Acc: 0.9963\n",
            "Client 26 -> Train Loss: 0.0337, Train Acc: 0.9907\n",
            "Round 11 -> Avg Train Loss: 0.0235, Avg Train Acc: 0.9954\n",
            "Round 11 -> Val Loss: 2.6443, Val Acc: 0.5883\n",
            "\n",
            "--- Federated Round 12/50 ---\n",
            "Client 22 -> Train Loss: 0.0311, Train Acc: 0.9938\n",
            "Client 25 -> Train Loss: 0.0092, Train Acc: 1.0000\n",
            "Client 11 -> Train Loss: 0.0540, Train Acc: 0.9874\n",
            "Client 13 -> Train Loss: 0.0095, Train Acc: 0.9985\n",
            "Client 2 -> Train Loss: 0.0091, Train Acc: 1.0000\n",
            "Round 12 -> Avg Train Loss: 0.0226, Avg Train Acc: 0.9960\n",
            "Round 12 -> Val Loss: 2.4953, Val Acc: 0.5991\n",
            "\n",
            "--- Federated Round 13/50 ---\n",
            "Client 21 -> Train Loss: 0.0111, Train Acc: 1.0000\n",
            "Client 29 -> Train Loss: 0.0574, Train Acc: 0.9898\n",
            "Client 48 -> Train Loss: 0.0297, Train Acc: 0.9943\n",
            "Client 34 -> Train Loss: 0.0132, Train Acc: 0.9980\n",
            "Client 2 -> Train Loss: 0.0082, Train Acc: 1.0000\n",
            "Round 13 -> Avg Train Loss: 0.0239, Avg Train Acc: 0.9964\n",
            "Round 13 -> Val Loss: 2.5075, Val Acc: 0.5912\n",
            "\n",
            "--- Federated Round 14/50 ---\n",
            "Client 38 -> Train Loss: 0.0063, Train Acc: 1.0000\n",
            "Client 37 -> Train Loss: 0.0106, Train Acc: 0.9966\n",
            "Client 50 -> Train Loss: 0.0287, Train Acc: 0.9971\n",
            "Client 42 -> Train Loss: 0.0292, Train Acc: 0.9970\n",
            "Client 49 -> Train Loss: 0.0261, Train Acc: 0.9955\n",
            "Round 14 -> Avg Train Loss: 0.0202, Avg Train Acc: 0.9972\n",
            "Round 14 -> Val Loss: 2.3376, Val Acc: 0.5937\n",
            "\n",
            "--- Federated Round 15/50 ---\n",
            "Client 44 -> Train Loss: 0.0072, Train Acc: 1.0000\n",
            "Client 41 -> Train Loss: 0.0301, Train Acc: 0.9956\n",
            "Client 31 -> Train Loss: 0.0280, Train Acc: 0.9943\n",
            "Client 1 -> Train Loss: 0.0170, Train Acc: 0.9948\n",
            "Client 11 -> Train Loss: 0.0426, Train Acc: 0.9909\n",
            "Round 15 -> Avg Train Loss: 0.0250, Avg Train Acc: 0.9951\n",
            "Round 15 -> Val Loss: 2.2732, Val Acc: 0.6069\n",
            "\n",
            "--- Federated Round 16/50 ---\n",
            "Client 26 -> Train Loss: 0.0227, Train Acc: 0.9954\n",
            "Client 14 -> Train Loss: 0.0147, Train Acc: 0.9988\n",
            "Client 50 -> Train Loss: 0.0238, Train Acc: 0.9956\n",
            "Client 30 -> Train Loss: 0.0625, Train Acc: 0.9848\n",
            "Client 31 -> Train Loss: 0.0240, Train Acc: 0.9966\n",
            "Round 16 -> Avg Train Loss: 0.0295, Avg Train Acc: 0.9942\n",
            "Round 16 -> Val Loss: 2.3240, Val Acc: 0.6047\n",
            "\n",
            "--- Federated Round 17/50 ---\n",
            "Client 44 -> Train Loss: 0.0072, Train Acc: 1.0000\n",
            "Client 33 -> Train Loss: 0.0151, Train Acc: 1.0000\n",
            "Client 28 -> Train Loss: 0.0417, Train Acc: 0.9926\n",
            "Client 45 -> Train Loss: 0.0212, Train Acc: 0.9966\n",
            "Client 42 -> Train Loss: 0.0264, Train Acc: 0.9985\n",
            "Round 17 -> Avg Train Loss: 0.0223, Avg Train Acc: 0.9975\n",
            "Round 17 -> Val Loss: 2.2471, Val Acc: 0.6040\n",
            "\n",
            "--- Federated Round 18/50 ---\n",
            "Client 16 -> Train Loss: 0.0099, Train Acc: 0.9991\n",
            "Client 32 -> Train Loss: 0.0599, Train Acc: 0.9827\n",
            "Client 9 -> Train Loss: 0.0168, Train Acc: 0.9951\n",
            "Client 48 -> Train Loss: 0.0229, Train Acc: 0.9966\n",
            "Client 24 -> Train Loss: 0.0103, Train Acc: 0.9990\n",
            "Round 18 -> Avg Train Loss: 0.0240, Avg Train Acc: 0.9945\n",
            "Round 18 -> Val Loss: 2.2217, Val Acc: 0.6049\n",
            "\n",
            "--- Federated Round 19/50 ---\n",
            "Client 2 -> Train Loss: 0.0059, Train Acc: 1.0000\n",
            "Client 23 -> Train Loss: 0.0074, Train Acc: 1.0000\n",
            "Client 47 -> Train Loss: 0.0227, Train Acc: 0.9972\n",
            "Client 21 -> Train Loss: 0.0103, Train Acc: 1.0000\n",
            "Client 42 -> Train Loss: 0.0254, Train Acc: 0.9985\n",
            "Round 19 -> Avg Train Loss: 0.0143, Avg Train Acc: 0.9991\n",
            "Round 19 -> Val Loss: 2.3270, Val Acc: 0.6065\n",
            "\n",
            "--- Federated Round 20/50 ---\n",
            "Client 33 -> Train Loss: 0.0134, Train Acc: 1.0000\n",
            "Client 14 -> Train Loss: 0.0155, Train Acc: 0.9975\n",
            "Client 18 -> Train Loss: 0.0170, Train Acc: 0.9969\n",
            "Client 6 -> Train Loss: 0.0218, Train Acc: 0.9956\n",
            "Client 23 -> Train Loss: 0.0085, Train Acc: 1.0000\n",
            "Round 20 -> Avg Train Loss: 0.0152, Avg Train Acc: 0.9980\n",
            "Round 20 -> Val Loss: 2.3891, Val Acc: 0.6010\n",
            "\n",
            "--- Federated Round 21/50 ---\n",
            "Client 27 -> Train Loss: 0.0322, Train Acc: 0.9928\n",
            "Client 10 -> Train Loss: 0.0531, Train Acc: 0.9907\n",
            "Client 13 -> Train Loss: 0.0068, Train Acc: 0.9993\n",
            "Client 45 -> Train Loss: 0.0185, Train Acc: 1.0000\n",
            "Client 47 -> Train Loss: 0.0182, Train Acc: 0.9972\n",
            "Round 21 -> Avg Train Loss: 0.0258, Avg Train Acc: 0.9960\n",
            "Round 21 -> Val Loss: 2.2895, Val Acc: 0.6060\n",
            "\n",
            "--- Federated Round 22/50 ---\n",
            "Client 1 -> Train Loss: 0.0162, Train Acc: 0.9987\n",
            "Client 10 -> Train Loss: 0.0446, Train Acc: 0.9960\n",
            "Client 49 -> Train Loss: 0.0216, Train Acc: 0.9973\n",
            "Client 22 -> Train Loss: 0.0248, Train Acc: 0.9975\n",
            "Client 15 -> Train Loss: 0.0193, Train Acc: 0.9951\n",
            "Round 22 -> Avg Train Loss: 0.0253, Avg Train Acc: 0.9969\n",
            "Round 22 -> Val Loss: 2.2045, Val Acc: 0.6117\n",
            "\n",
            "--- Federated Round 23/50 ---\n",
            "Client 33 -> Train Loss: 0.0110, Train Acc: 1.0000\n",
            "Client 35 -> Train Loss: 0.0282, Train Acc: 0.9975\n",
            "Client 43 -> Train Loss: 0.0337, Train Acc: 0.9966\n",
            "Client 1 -> Train Loss: 0.0143, Train Acc: 0.9987\n",
            "Client 16 -> Train Loss: 0.0075, Train Acc: 0.9991\n",
            "Round 23 -> Avg Train Loss: 0.0189, Avg Train Acc: 0.9984\n",
            "Round 23 -> Val Loss: 2.2014, Val Acc: 0.6121\n",
            "\n",
            "--- Federated Round 24/50 ---\n",
            "Client 8 -> Train Loss: 0.0233, Train Acc: 0.9944\n",
            "Client 21 -> Train Loss: 0.0095, Train Acc: 1.0000\n",
            "Client 20 -> Train Loss: 0.0512, Train Acc: 0.9856\n",
            "Client 7 -> Train Loss: 0.0293, Train Acc: 0.9948\n",
            "Client 16 -> Train Loss: 0.0068, Train Acc: 1.0000\n",
            "Round 24 -> Avg Train Loss: 0.0240, Avg Train Acc: 0.9950\n",
            "Round 24 -> Val Loss: 2.1314, Val Acc: 0.6073\n",
            "\n",
            "--- Federated Round 25/50 ---\n",
            "Client 14 -> Train Loss: 0.0130, Train Acc: 0.9988\n",
            "Client 40 -> Train Loss: 0.0565, Train Acc: 0.9864\n",
            "Client 4 -> Train Loss: 0.0259, Train Acc: 0.9987\n",
            "Client 2 -> Train Loss: 0.0049, Train Acc: 1.0000\n",
            "Client 50 -> Train Loss: 0.0205, Train Acc: 0.9985\n",
            "Round 25 -> Avg Train Loss: 0.0242, Avg Train Acc: 0.9965\n",
            "Round 25 -> Val Loss: 2.1810, Val Acc: 0.6138\n",
            "\n",
            "--- Federated Round 26/50 ---\n",
            "Client 42 -> Train Loss: 0.0190, Train Acc: 0.9985\n",
            "Client 29 -> Train Loss: 0.0410, Train Acc: 0.9962\n",
            "Client 37 -> Train Loss: 0.0047, Train Acc: 1.0000\n",
            "Client 1 -> Train Loss: 0.0122, Train Acc: 1.0000\n",
            "Client 20 -> Train Loss: 0.0439, Train Acc: 0.9896\n",
            "Round 26 -> Avg Train Loss: 0.0242, Avg Train Acc: 0.9968\n",
            "Round 26 -> Val Loss: 2.1005, Val Acc: 0.6163\n",
            "\n",
            "--- Federated Round 27/50 ---\n",
            "Client 7 -> Train Loss: 0.0254, Train Acc: 0.9961\n",
            "Client 9 -> Train Loss: 0.0121, Train Acc: 0.9984\n",
            "Client 24 -> Train Loss: 0.0082, Train Acc: 1.0000\n",
            "Client 21 -> Train Loss: 0.0081, Train Acc: 1.0000\n",
            "Client 1 -> Train Loss: 0.0116, Train Acc: 1.0000\n",
            "Round 27 -> Avg Train Loss: 0.0131, Avg Train Acc: 0.9989\n",
            "Round 27 -> Val Loss: 2.1610, Val Acc: 0.6107\n",
            "\n",
            "--- Federated Round 28/50 ---\n",
            "Client 15 -> Train Loss: 0.0114, Train Acc: 0.9984\n",
            "Client 16 -> Train Loss: 0.0056, Train Acc: 1.0000\n",
            "Client 38 -> Train Loss: 0.0061, Train Acc: 1.0000\n",
            "Client 30 -> Train Loss: 0.0489, Train Acc: 0.9931\n",
            "Client 18 -> Train Loss: 0.0119, Train Acc: 0.9979\n",
            "Round 28 -> Avg Train Loss: 0.0168, Avg Train Acc: 0.9979\n",
            "Round 28 -> Val Loss: 2.1113, Val Acc: 0.6154\n",
            "\n",
            "--- Federated Round 29/50 ---\n",
            "Client 14 -> Train Loss: 0.0124, Train Acc: 0.9988\n",
            "Client 24 -> Train Loss: 0.0076, Train Acc: 1.0000\n",
            "Client 42 -> Train Loss: 0.0180, Train Acc: 0.9985\n",
            "Client 23 -> Train Loss: 0.0069, Train Acc: 1.0000\n",
            "Client 43 -> Train Loss: 0.0265, Train Acc: 0.9977\n",
            "Round 29 -> Avg Train Loss: 0.0143, Avg Train Acc: 0.9990\n",
            "Round 29 -> Val Loss: 2.1626, Val Acc: 0.6151\n",
            "\n",
            "--- Federated Round 30/50 ---\n",
            "Client 33 -> Train Loss: 0.0097, Train Acc: 1.0000\n",
            "Client 19 -> Train Loss: 0.0060, Train Acc: 1.0000\n",
            "Client 9 -> Train Loss: 0.0101, Train Acc: 1.0000\n",
            "Client 29 -> Train Loss: 0.0291, Train Acc: 0.9987\n",
            "Client 2 -> Train Loss: 0.0050, Train Acc: 1.0000\n",
            "Round 30 -> Avg Train Loss: 0.0120, Avg Train Acc: 0.9997\n",
            "Round 30 -> Val Loss: 2.1996, Val Acc: 0.6149\n",
            "\n",
            "--- Federated Round 31/50 ---\n",
            "Client 34 -> Train Loss: 0.0072, Train Acc: 1.0000\n",
            "Client 6 -> Train Loss: 0.0113, Train Acc: 0.9967\n",
            "Client 43 -> Train Loss: 0.0217, Train Acc: 1.0000\n",
            "Client 47 -> Train Loss: 0.0119, Train Acc: 0.9986\n",
            "Client 14 -> Train Loss: 0.0128, Train Acc: 0.9988\n",
            "Round 31 -> Avg Train Loss: 0.0130, Avg Train Acc: 0.9988\n",
            "Round 31 -> Val Loss: 2.1938, Val Acc: 0.6097\n",
            "\n",
            "--- Federated Round 32/50 ---\n",
            "Client 15 -> Train Loss: 0.0086, Train Acc: 1.0000\n",
            "Client 17 -> Train Loss: 0.0755, Train Acc: 0.9772\n",
            "Client 35 -> Train Loss: 0.0201, Train Acc: 0.9975\n",
            "Client 32 -> Train Loss: 0.0369, Train Acc: 0.9907\n",
            "Client 21 -> Train Loss: 0.0076, Train Acc: 1.0000\n",
            "Round 32 -> Avg Train Loss: 0.0297, Avg Train Acc: 0.9931\n",
            "Round 32 -> Val Loss: 2.1898, Val Acc: 0.6091\n",
            "\n",
            "--- Federated Round 33/50 ---\n",
            "Client 6 -> Train Loss: 0.0106, Train Acc: 0.9989\n",
            "Client 10 -> Train Loss: 0.0283, Train Acc: 0.9960\n",
            "Client 43 -> Train Loss: 0.0191, Train Acc: 1.0000\n",
            "Client 29 -> Train Loss: 0.0246, Train Acc: 0.9987\n",
            "Client 49 -> Train Loss: 0.0153, Train Acc: 0.9991\n",
            "Round 33 -> Avg Train Loss: 0.0196, Avg Train Acc: 0.9985\n",
            "Round 33 -> Val Loss: 2.2184, Val Acc: 0.6008\n",
            "\n",
            "--- Federated Round 34/50 ---\n",
            "Client 16 -> Train Loss: 0.0041, Train Acc: 1.0000\n",
            "Client 12 -> Train Loss: 0.0166, Train Acc: 0.9963\n",
            "Client 13 -> Train Loss: 0.0048, Train Acc: 1.0000\n",
            "Client 35 -> Train Loss: 0.0187, Train Acc: 0.9975\n",
            "Client 10 -> Train Loss: 0.0254, Train Acc: 0.9987\n",
            "Round 34 -> Avg Train Loss: 0.0139, Avg Train Acc: 0.9985\n",
            "Round 34 -> Val Loss: 2.1249, Val Acc: 0.6163\n",
            "\n",
            "--- Federated Round 35/50 ---\n",
            "Client 27 -> Train Loss: 0.0235, Train Acc: 0.9964\n",
            "Client 4 -> Train Loss: 0.0197, Train Acc: 1.0000\n",
            "Client 48 -> Train Loss: 0.0150, Train Acc: 0.9989\n",
            "Client 15 -> Train Loss: 0.0076, Train Acc: 1.0000\n",
            "Client 25 -> Train Loss: 0.0077, Train Acc: 1.0000\n",
            "Round 35 -> Avg Train Loss: 0.0147, Avg Train Acc: 0.9991\n",
            "Round 35 -> Val Loss: 2.0466, Val Acc: 0.6134\n",
            "\n",
            "--- Federated Round 36/50 ---\n",
            "Client 20 -> Train Loss: 0.0315, Train Acc: 0.9961\n",
            "Client 39 -> Train Loss: 0.0115, Train Acc: 1.0000\n",
            "Client 9 -> Train Loss: 0.0071, Train Acc: 1.0000\n",
            "Client 46 -> Train Loss: 0.0362, Train Acc: 0.9918\n",
            "Client 41 -> Train Loss: 0.0184, Train Acc: 0.9991\n",
            "Round 36 -> Avg Train Loss: 0.0209, Avg Train Acc: 0.9974\n",
            "Round 36 -> Val Loss: 1.9970, Val Acc: 0.6225\n",
            "\n",
            "--- Federated Round 37/50 ---\n",
            "Client 30 -> Train Loss: 0.0372, Train Acc: 0.9959\n",
            "Client 38 -> Train Loss: 0.0052, Train Acc: 1.0000\n",
            "Client 13 -> Train Loss: 0.0051, Train Acc: 1.0000\n",
            "Client 35 -> Train Loss: 0.0173, Train Acc: 0.9988\n",
            "Client 4 -> Train Loss: 0.0205, Train Acc: 1.0000\n",
            "Round 37 -> Avg Train Loss: 0.0171, Avg Train Acc: 0.9989\n",
            "Round 37 -> Val Loss: 1.9995, Val Acc: 0.6199\n",
            "\n",
            "--- Federated Round 38/50 ---\n",
            "Client 15 -> Train Loss: 0.0065, Train Acc: 1.0000\n",
            "Client 45 -> Train Loss: 0.0094, Train Acc: 1.0000\n",
            "Client 48 -> Train Loss: 0.0138, Train Acc: 1.0000\n",
            "Client 5 -> Train Loss: 0.0136, Train Acc: 0.9978\n",
            "Client 6 -> Train Loss: 0.0082, Train Acc: 0.9989\n",
            "Round 38 -> Avg Train Loss: 0.0103, Avg Train Acc: 0.9993\n",
            "Round 38 -> Val Loss: 2.0532, Val Acc: 0.6214\n",
            "\n",
            "--- Federated Round 39/50 ---\n",
            "Client 25 -> Train Loss: 0.0069, Train Acc: 1.0000\n",
            "Client 14 -> Train Loss: 0.0096, Train Acc: 0.9988\n",
            "Client 50 -> Train Loss: 0.0146, Train Acc: 1.0000\n",
            "Client 36 -> Train Loss: 0.0145, Train Acc: 0.9975\n",
            "Client 9 -> Train Loss: 0.0069, Train Acc: 1.0000\n",
            "Round 39 -> Avg Train Loss: 0.0105, Avg Train Acc: 0.9993\n",
            "Round 39 -> Val Loss: 2.0649, Val Acc: 0.6219\n",
            "\n",
            "--- Federated Round 40/50 ---\n",
            "Client 38 -> Train Loss: 0.0050, Train Acc: 1.0000\n",
            "Client 33 -> Train Loss: 0.0081, Train Acc: 1.0000\n",
            "Client 49 -> Train Loss: 0.0133, Train Acc: 0.9991\n",
            "Client 19 -> Train Loss: 0.0053, Train Acc: 1.0000\n",
            "Client 26 -> Train Loss: 0.0212, Train Acc: 0.9977\n",
            "Round 40 -> Avg Train Loss: 0.0106, Avg Train Acc: 0.9994\n",
            "Round 40 -> Val Loss: 2.0464, Val Acc: 0.6190\n",
            "\n",
            "--- Federated Round 41/50 ---\n",
            "Client 43 -> Train Loss: 0.0145, Train Acc: 1.0000\n",
            "Client 34 -> Train Loss: 0.0052, Train Acc: 1.0000\n",
            "Client 40 -> Train Loss: 0.0425, Train Acc: 0.9937\n",
            "Client 41 -> Train Loss: 0.0158, Train Acc: 0.9991\n",
            "Client 44 -> Train Loss: 0.0039, Train Acc: 1.0000\n",
            "Round 41 -> Avg Train Loss: 0.0164, Avg Train Acc: 0.9986\n",
            "Round 41 -> Val Loss: 2.0917, Val Acc: 0.6214\n",
            "\n",
            "--- Federated Round 42/50 ---\n",
            "Client 31 -> Train Loss: 0.0111, Train Acc: 0.9989\n",
            "Client 43 -> Train Loss: 0.0151, Train Acc: 1.0000\n",
            "Client 36 -> Train Loss: 0.0081, Train Acc: 1.0000\n",
            "Client 10 -> Train Loss: 0.0202, Train Acc: 0.9987\n",
            "Client 3 -> Train Loss: 0.0071, Train Acc: 0.9990\n",
            "Round 42 -> Avg Train Loss: 0.0123, Avg Train Acc: 0.9993\n",
            "Round 42 -> Val Loss: 2.0571, Val Acc: 0.6312\n",
            "\n",
            "--- Federated Round 43/50 ---\n",
            "Client 35 -> Train Loss: 0.0141, Train Acc: 1.0000\n",
            "Client 14 -> Train Loss: 0.0087, Train Acc: 1.0000\n",
            "Client 50 -> Train Loss: 0.0140, Train Acc: 1.0000\n",
            "Client 22 -> Train Loss: 0.0148, Train Acc: 0.9975\n",
            "Client 1 -> Train Loss: 0.0089, Train Acc: 1.0000\n",
            "Round 43 -> Avg Train Loss: 0.0121, Avg Train Acc: 0.9995\n",
            "Round 43 -> Val Loss: 2.1342, Val Acc: 0.6246\n",
            "\n",
            "--- Federated Round 44/50 ---\n",
            "Client 8 -> Train Loss: 0.0112, Train Acc: 0.9992\n",
            "Client 14 -> Train Loss: 0.0105, Train Acc: 1.0000\n",
            "Client 50 -> Train Loss: 0.0134, Train Acc: 1.0000\n",
            "Client 15 -> Train Loss: 0.0055, Train Acc: 1.0000\n",
            "Client 7 -> Train Loss: 0.0148, Train Acc: 1.0000\n",
            "Round 44 -> Avg Train Loss: 0.0111, Avg Train Acc: 0.9998\n",
            "Round 44 -> Val Loss: 2.0736, Val Acc: 0.6191\n",
            "\n",
            "--- Federated Round 45/50 ---\n",
            "Client 9 -> Train Loss: 0.0056, Train Acc: 1.0000\n",
            "Client 13 -> Train Loss: 0.0038, Train Acc: 1.0000\n",
            "Client 10 -> Train Loss: 0.0190, Train Acc: 1.0000\n",
            "Client 19 -> Train Loss: 0.0043, Train Acc: 1.0000\n",
            "Client 30 -> Train Loss: 0.0317, Train Acc: 0.9972\n",
            "Round 45 -> Avg Train Loss: 0.0129, Avg Train Acc: 0.9994\n",
            "Round 45 -> Val Loss: 2.0672, Val Acc: 0.6244\n",
            "\n",
            "--- Federated Round 46/50 ---\n",
            "Client 38 -> Train Loss: 0.0040, Train Acc: 1.0000\n",
            "Client 19 -> Train Loss: 0.0052, Train Acc: 1.0000\n",
            "Client 20 -> Train Loss: 0.0241, Train Acc: 0.9987\n",
            "Client 46 -> Train Loss: 0.0288, Train Acc: 0.9941\n",
            "Client 14 -> Train Loss: 0.0099, Train Acc: 1.0000\n",
            "Round 46 -> Avg Train Loss: 0.0144, Avg Train Acc: 0.9986\n",
            "Round 46 -> Val Loss: 2.0503, Val Acc: 0.6192\n",
            "\n",
            "--- Federated Round 47/50 ---\n",
            "Client 22 -> Train Loss: 0.0121, Train Acc: 1.0000\n",
            "Client 11 -> Train Loss: 0.0285, Train Acc: 0.9966\n",
            "Client 33 -> Train Loss: 0.0082, Train Acc: 1.0000\n",
            "Client 6 -> Train Loss: 0.0061, Train Acc: 1.0000\n",
            "Client 31 -> Train Loss: 0.0098, Train Acc: 1.0000\n",
            "Round 47 -> Avg Train Loss: 0.0129, Avg Train Acc: 0.9993\n",
            "Round 47 -> Val Loss: 2.0670, Val Acc: 0.6212\n",
            "\n",
            "--- Federated Round 48/50 ---\n",
            "Client 18 -> Train Loss: 0.0082, Train Acc: 0.9990\n",
            "Client 38 -> Train Loss: 0.0042, Train Acc: 1.0000\n",
            "Client 29 -> Train Loss: 0.0189, Train Acc: 0.9987\n",
            "Client 21 -> Train Loss: 0.0057, Train Acc: 1.0000\n",
            "Client 42 -> Train Loss: 0.0117, Train Acc: 0.9985\n",
            "Round 48 -> Avg Train Loss: 0.0098, Avg Train Acc: 0.9992\n",
            "Round 48 -> Val Loss: 2.0431, Val Acc: 0.6218\n",
            "\n",
            "--- Federated Round 49/50 ---\n",
            "Client 2 -> Train Loss: 0.0037, Train Acc: 1.0000\n",
            "Client 49 -> Train Loss: 0.0110, Train Acc: 1.0000\n",
            "Client 7 -> Train Loss: 0.0115, Train Acc: 1.0000\n",
            "Client 36 -> Train Loss: 0.0068, Train Acc: 1.0000\n",
            "Client 14 -> Train Loss: 0.0090, Train Acc: 1.0000\n",
            "Round 49 -> Avg Train Loss: 0.0084, Avg Train Acc: 1.0000\n",
            "Round 49 -> Val Loss: 2.1801, Val Acc: 0.6193\n",
            "\n",
            "--- Federated Round 50/50 ---\n",
            "Client 1 -> Train Loss: 0.0081, Train Acc: 1.0000\n",
            "Client 2 -> Train Loss: 0.0050, Train Acc: 1.0000\n",
            "Client 35 -> Train Loss: 0.0141, Train Acc: 1.0000\n",
            "Client 9 -> Train Loss: 0.0059, Train Acc: 1.0000\n",
            "Client 23 -> Train Loss: 0.0046, Train Acc: 1.0000\n",
            "Round 50 -> Avg Train Loss: 0.0075, Avg Train Acc: 1.0000\n",
            "Round 50 -> Val Loss: 2.3549, Val Acc: 0.6076\n"
          ]
        }
      ],
      "source": [
        "set_seeds(123)\n",
        "\n",
        "# === LOSS FUNCTION ===\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# === START TIMER ===\n",
        "start_time = time.time()\n",
        "\n",
        "# === CALIBRATION MASK ===\n",
        "start_c_time = time.time()\n",
        "print(f'Masking, selection method: {KEEP}')\n",
        "calibrate_all_clients(\n",
        "    clients, \n",
        "    imported_model_train, \n",
        "    device, \n",
        "    R=C_ROUNDS, \n",
        "    final_sparsity=SPARSITY, \n",
        "    lr=LR, \n",
        "    keep=KEEP\n",
        ")\n",
        "end_c_time = time.time()\n",
        "c_time = round(end_c_time - start_c_time, 2)\n",
        "\n",
        "\n",
        "print(\"Calcolating global Fisher scores before FL training...\")\n",
        "global_fisher_scores = compute_global_fisher(clients, imported_model_train, device)\n",
        "\n",
        " \n",
        "# === TRAIN FL ===\n",
        "best_global_model, test_losses, test_accuracies, avg_train_losses, avg_train_accuracies = train_test_model_FL(\n",
        "    global_model=imported_model_train,\n",
        "    criterion=criterion,\n",
        "    LR=LR,\n",
        "    MOMENTUM=MOMENTUM,\n",
        "    WEIGHT_DECAY=WEIGHT_DECAY,\n",
        "    T_MAX=T_MAX,\n",
        "    clients=clients,\n",
        "    val_test_loader=test_loader,\n",
        "    num_epochs=N_EP,\n",
        "    C=C,\n",
        "    J=J,\n",
        "    checkpoint_path=checkpoint_path,\n",
        "    checkpoints=True,\n",
        "    verbose=VERBOSE,\n",
        "    use_sparse=True,     # Using sparse SGD\n",
        "    global_fisher_scores=global_fisher_scores\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = round(end_time - start_time, 2)\n",
        "\n",
        "# === SAVING RESULTS ===\n",
        "results_FL_sparse = {\n",
        "    \"flag\": FLAG,\n",
        "    \"J\": J,\n",
        "    \"Nc\": Nc,\n",
        "    \"epochs\": N_EP,\n",
        "    \"c_rounds\": C_ROUNDS,\n",
        "    \"c_time\": c_time,\n",
        "    \"sparsity\": SPARSITY,\n",
        "    \"avg_train_losses\": avg_train_losses,\n",
        "    \"avg_train_accuracies\": avg_train_accuracies,\n",
        "    \"test_losses\": test_losses,\n",
        "    \"test_accuracies\": test_accuracies,\n",
        "    \"training_time_sec\": training_time,\n",
        "    \"global_fisher_scores\": global_fisher_scores   # <-- addition\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "791a045f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ global_fisher_scores rimosso da results_FL_sparse\n"
          ]
        }
      ],
      "source": [
        "# Remove the \"global_fisher_scores\" field if present\n",
        "if \"global_fisher_scores\" in results_FL_sparse:\n",
        "    del results_FL_sparse[\"global_fisher_scores\"]\n",
        "    print(\"global_fisher_scores removed from results_FL_sparse\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8653d38d",
      "metadata": {
        "id": "8653d38d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ JSON salvato: results_FL_FINE_TUNED_niid_J4_Nc5_Cal2_S0.5_EXTRA.json\n",
            "✅ Modello salvato: global_model_FL_FINE_TUNED_niid_J4_Nc5_Cal2_S0.5_EXTRA.pth\n"
          ]
        }
      ],
      "source": [
        "# === SAVING JSON ===\n",
        "json_filename = f\"results_FL_FINE_TUNED_{FLAG}_J{J}_Nc{Nc}_Cal{C_ROUNDS}_S{SPARSITY}_EXTRA.json\"\n",
        "with open(json_filename, 'w') as f:\n",
        "    json.dump(results_FL_sparse, f, indent=2)\n",
        "\n",
        "# === SAVING WEIGHTS ===\n",
        "model_filename = f\"global_model_FL_FINE_TUNED_{FLAG}_J{J}_Nc{Nc}_Cal{C_ROUNDS}_S{SPARSITY}_EXTRA.pth\"\n",
        "torch.save(best_global_model.state_dict(), model_filename)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "a50b02b4",
        "8ddb98ed",
        "34e82ddf",
        "e5007e58",
        "17af90e4",
        "8e886bc0",
        "e9fdaad5",
        "d6cb5b9d",
        "ed3454ae",
        "2b6dde94"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
