{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84300387",
      "metadata": {
        "id": "84300387"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import Subset\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from collections import OrderedDict\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "import wandb\n",
        "import json\n",
        "import heapq\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "\n",
        "\n",
        "# Seeds for reproducibility\n",
        "def set_seeds(seed: int = 123):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seeds(123)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f9f76a6",
      "metadata": {
        "id": "7f9f76a6"
      },
      "outputs": [],
      "source": [
        "\"\"\" PLOT SETTINGS \"\"\"\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "plt.rcParams.update({\n",
        "    \"font.size\": 18,            # base font size\n",
        "    \"axes.titlesize\": 24,       # axis titles\n",
        "    \"axes.labelsize\": 22,       # axis labels\n",
        "    \"xtick.labelsize\": 18,      # X axis numbers\n",
        "    \"ytick.labelsize\": 18,      # Y axis numbers\n",
        "    \"legend.fontsize\": 18,      # legend text\n",
        "    \"lines.linewidth\": 3.0      # line thickness\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85eba51c",
      "metadata": {
        "id": "85eba51c"
      },
      "outputs": [],
      "source": [
        "# --- Define DINOWithHead ---\n",
        "class DINOWithHead(nn.Module):\n",
        "    def __init__(self, backbone, num_classes=100, p=None):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        layers = []\n",
        "        if p is not None:\n",
        "            layers.append(nn.Dropout(p=p))\n",
        "        layers.append(nn.Linear(384, num_classes))\n",
        "        self.head = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        out = self.head(features)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a50b02b4",
      "metadata": {
        "id": "a50b02b4"
      },
      "source": [
        "## Data import and loader creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0326f737",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0326f737",
        "outputId": "65327a09-721b-4418-f318-34c8ff7ce748"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169M/169M [00:15<00:00, 10.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "\"\"\" DATASET DOWNLOAD \"\"\"\n",
        "ROOT = './data'\n",
        "BATCH_SIZE=64\n",
        "#BATCH_SIZE = 128\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "tot_train_data = torchvision.datasets.CIFAR100(root=ROOT, train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
        "test_data = torchvision.datasets.CIFAR100(root=ROOT, train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" SPLIT TOT_TRAININ in VALIDATION and TRAIN \"\"\"\n",
        "\n",
        "def split_dataset(tot_train_data, valid_ratio=0.8):\n",
        "    \"\"\"\n",
        "    Splits the given dataset randomly into training and validation subsets\n",
        "    \"\"\"\n",
        "    train_size = int(valid_ratio * len(tot_train_data))\n",
        "    val_size = len(tot_train_data) - train_size\n",
        "    train_data, val_data = random_split(tot_train_data, [train_size, val_size])\n",
        "    return train_data, val_data\n",
        "\n",
        "train_data, val_data = split_dataset(tot_train_data, valid_ratio=0.8)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" DATA TRANSFORMATION \"\"\"\n",
        "\n",
        "def data_trasform(dataset, data_augmentation=False):   ### train_data or tot_train_data\n",
        "    \"\"\"\n",
        "    Returns train and val/test transforms based on dataset stats.\n",
        "    Dataset (for computing mean and std) can be either training only or combined train+validation.\n",
        "\n",
        "    If data_augmentation=True, applies augmentation on training transforms, otherwise only resize and normalize.\n",
        "    \"\"\"\n",
        "\n",
        "    # MEAN and VARIANCE (considering 3 channels)\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "    nb_samples = 0\n",
        "\n",
        "    for img, _ in dataset:\n",
        "        img = img.view(3, -1)  # Flatten H*W\n",
        "        mean += img.mean(1)\n",
        "        std += img.std(1)\n",
        "        nb_samples += 1\n",
        "\n",
        "    mean /= nb_samples\n",
        "    std /= nb_samples\n",
        "\n",
        "\n",
        "    if data_augmentation:\n",
        "        train_transforms = transforms.Compose([\n",
        "            transforms.Resize(64, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "            transforms.RandomCrop(64, padding=4),\n",
        "            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(degrees=15),\n",
        "            transforms.RandAugment(num_ops=2, magnitude=9),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean, std=std)\n",
        "        ])\n",
        "    else:\n",
        "        train_transforms = transforms.Compose([\n",
        "            transforms.Resize(64, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean, std=std)\n",
        "        ])\n",
        "\n",
        "    ### NO DATA AUGMENTATION!\n",
        "    val_test_transforms = transforms.Compose([\n",
        "        transforms.Resize(64),                    # Resize\n",
        "        transforms.ToTensor(),                     # Convert to tensor\n",
        "        transforms.Normalize(mean=mean, std=std)   # Normalization using the training statistics\n",
        "    ])\n",
        "\n",
        "\n",
        "    return train_transforms, val_test_transforms\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" DATA TRANSFORMATION and LOADERS \"\"\"\n",
        "\n",
        "### ===== For hyperparameter tuning considering train_data and val_data =====\n",
        "train_transforms, val_test_transforms = data_trasform(train_data)\n",
        "\n",
        "train_data.dataset.transform = train_transforms\n",
        "val_data.dataset.transform = val_test_transforms\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "val_loader   = DataLoader(val_data,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "\n",
        "\n",
        "### ===== For model testing considering tot_train_data and test_data =====\n",
        "train_transforms, val_test_transforms = data_trasform(tot_train_data)\n",
        "\n",
        "tot_train_data = torchvision.datasets.CIFAR100(root=ROOT, train=True, download=False, transform=train_transforms)\n",
        "test_data = torchvision.datasets.CIFAR100(root=ROOT, train=False, download=False, transform=val_test_transforms)\n",
        "\n",
        "tot_train_loader = DataLoader(tot_train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ddb98ed",
      "metadata": {
        "id": "8ddb98ed"
      },
      "source": [
        "## Functions Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeba326e",
      "metadata": {
        "id": "aeba326e"
      },
      "outputs": [],
      "source": [
        "\"\"\" SPLIT INTO K CLIENTS: iid and non-iid distributions  \"\"\"\n",
        "\n",
        "def iid_shard(dataset, K):\n",
        "    \"\"\"\n",
        "    Splits the dataset into K disjoint subsets (one per client) in an IID fashion. Each client receives ~len(dataset)/K samples,\n",
        "    drawn uniformly at random across all classes.\n",
        "    \"\"\"\n",
        "    num_items_per_client = len(dataset) // K\n",
        "    remainder = len(dataset) % K\n",
        "    all_indices = list(range(len(dataset)))\n",
        "    random.shuffle(all_indices)\n",
        "\n",
        "    client_data = {}  # Creation of a dictionary for clients\n",
        "    start = 0\n",
        "    for i in range(K):\n",
        "        # give +1 to the first `remainder` clients\n",
        "        add = num_items_per_client + (1 if i < remainder else 0)\n",
        "        end = start + add\n",
        "        client_data[i+1] = Subset(dataset, all_indices[start:end])\n",
        "        start = end\n",
        "\n",
        "    return client_data\n",
        "\n",
        "\n",
        "def noniid_shard(dataset, K, Nc):\n",
        "    \"\"\"\n",
        "    Splits the dataset into K disjoint subsets (one per client) in a non-IID fashion.\n",
        "    Disjoint dataset: each sample must belong to exactly one client and cannot be duplicated.\n",
        "    Nc: each client receives samples from Nc distinct classes, to simulate variability of data across clients.\n",
        "    \"\"\"\n",
        "    # === Feasibility check ===\n",
        "    all_classes = set(label for _, label in dataset)\n",
        "    num_classes = len(all_classes)\n",
        "    if Nc > num_classes:\n",
        "        raise ValueError(f\"Cannot assign {Nc} classes per client: only {num_classes} classes available.\")\n",
        "\n",
        "\n",
        "    # === Step 1: Organize data by class ===\n",
        "    # This ensures we can select samples from specific classes without duplication.\n",
        "    label_to_indices = defaultdict(list)\n",
        "    for idx, (_, label) in enumerate(dataset):\n",
        "        label_to_indices[label].append(idx)    # key: class, value: list of idxs of the corresponding class\n",
        "\n",
        "    # === Step 2: Shuffle samples within each class ===\n",
        "    # Prevents consecutive samples from going to the same clients repeatedly.\n",
        "    for cls in label_to_indices:\n",
        "        random.shuffle(label_to_indices[cls])\n",
        "\n",
        "    # === Step 3: Prepare the list of clients and shuffle it ===\n",
        "    # Guarantees random assignment of classes to clients.\n",
        "    all_classes = list(label_to_indices.keys())\n",
        "\n",
        "    client_class_map = {}\n",
        "    for client_id in range(1, K+1):        # Pre-select Nc classes per client at the beginning.\n",
        "        client_class_map[client_id] = random.sample(all_classes, Nc)\n",
        "\n",
        "    client_data = {cid: [] for cid in range(1, K+1)}    # Initialize empty client datasets\n",
        "    class_to_clients = defaultdict(list)     # For each class, keep track of clients that have it (reverse map from class → clients that have that class)\n",
        "    for cid, classes in client_class_map.items():\n",
        "        for cls in classes:\n",
        "            class_to_clients[cls].append(cid)\n",
        "\n",
        "\n",
        "    # === Step 4: Distribute samples in rounds ===\n",
        "    # Distributing samples to clients in rounds\n",
        "    for cls, indices in label_to_indices.items():    # Iterate over all classes, where indices is the list of the samples for the considered class\n",
        "        clients = class_to_clients[cls]  # clients that want this class\n",
        "        if not clients:\n",
        "            continue\n",
        "        i = 0\n",
        "        while indices:\n",
        "            client_id = clients[i % len(clients)]  # cycling continuously through the list of clients\n",
        "            sample = indices.pop()                 # take one sample and it removes it so it will not be repeated\n",
        "            client_data[client_id].append(sample)  # assign to this client\n",
        "            i += 1\n",
        "\n",
        "     # === Step 5: Convert sample index lists into Subsets ===\n",
        "    for cid in client_data:\n",
        "        client_data[cid] = Subset(dataset, client_data[cid])\n",
        "\n",
        "    return client_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34e82ddf",
      "metadata": {
        "id": "34e82ddf"
      },
      "source": [
        "## Class Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff77908b",
      "metadata": {
        "id": "ff77908b"
      },
      "outputs": [],
      "source": [
        "def compress_mask(mask_dict):\n",
        "    \"\"\"\n",
        "    Convert each 0/1 tensor in the mask into a compressed bit array.\n",
        "    Returns a dictionary: {param_name: (packed_uint8_array, original_shape)}\n",
        "    \"\"\"\n",
        "    compressed = {}\n",
        "    for name, tensor in mask_dict.items():\n",
        "        # Move tensor to CPU, cast to uint8, flatten, and convert to numpy\n",
        "        arr = tensor.detach().to('cpu').to(torch.uint8).contiguous().view(-1).numpy()\n",
        "        # Pack bits into a compact representation (8x smaller than uint8)\n",
        "        packed = np.packbits(arr)\n",
        "        compressed[name] = (packed, tuple(tensor.shape))  # store packed data + original shape\n",
        "    return compressed\n",
        "\n",
        "\n",
        "def decompress_mask(compressed_dict):\n",
        "    \"\"\"\n",
        "    Reconstruct torch.uint8 tensors with the original shape (0/1 values).\n",
        "    \"\"\"\n",
        "    mask = {}\n",
        "    for name, (packed, shape) in compressed_dict.items():\n",
        "        total = int(np.prod(shape))  # number of elements in original tensor\n",
        "        # Unpack bits back to 0/1 and trim extra padding\n",
        "        unpacked = np.unpackbits(packed)[:total]\n",
        "        # Reshape to original tensor shape and convert to torch.uint8\n",
        "        mask[name] = torch.from_numpy(unpacked.reshape(shape)).to(torch.uint8)\n",
        "    return mask\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" CLASS CLIENT\"\"\"\n",
        "class Client:\n",
        "    def __init__(self, client_id, dataset, loader,\n",
        "                 mask=None, extra_mask=None, fisher_scores=None,\n",
        "                 local_weights=None, compressed_mask=None):\n",
        "        self.client_id = client_id\n",
        "        self.dataset = dataset\n",
        "        self.loader = loader\n",
        "\n",
        "        # Full mask in RAM (0/1 tensors) – only needed during training\n",
        "        self.mask = mask\n",
        "\n",
        "        # Lightweight storage version (bit-packed mask)\n",
        "        self.compressed_mask = compressed_mask\n",
        "\n",
        "        # Optional: additional mask (depends on your logic)\n",
        "        self.extra_mask = extra_mask\n",
        "\n",
        "        # Fisher scores are only needed during calibration;\n",
        "        # after that, you can set them to None to save memory\n",
        "        self.fisher_scores = fisher_scores\n",
        "\n",
        "        # Local weights are not really used in your training loop;\n",
        "        # kept here only for compatibility\n",
        "        self.local_weights = local_weights\n",
        "\n",
        "    def compress_mask(self):\n",
        "        \"\"\"\n",
        "        Compress self.mask into self.compressed_mask and free RAM\n",
        "        \"\"\"\n",
        "        if self.mask is not None:\n",
        "            # Make sure mask tensors are uint8 (0/1 values)\n",
        "            self.mask = {k: v.to(torch.uint8) for k, v in self.mask.items()}\n",
        "            self.compressed_mask = compress_mask(self.mask)\n",
        "            self.mask = None  # release memory\n",
        "\n",
        "    def decompress_mask(self):\n",
        "        \"\"\"\n",
        "        Rebuild self.mask (0/1 tensors) from self.compressed_mask\n",
        "        \"\"\"\n",
        "        if self.compressed_mask is not None and self.mask is None:\n",
        "            self.mask = decompress_mask(self.compressed_mask)\n",
        "\n",
        "\n",
        "\n",
        "    def num_samples(self):\n",
        "        \"\"\"\n",
        "        Returns the number of samples in the client’s local dataset.\n",
        "        \"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    @staticmethod\n",
        "    def print_samples(clients):\n",
        "        \"\"\"\n",
        "        Prints the number of samples each client holds.\n",
        "        \"\"\"\n",
        "        print(\"Number of samples per client:\")\n",
        "        for client in clients:\n",
        "            print(f\"Client {client.client_id}: {client.num_samples()} samples\")\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_class_distribution(clients, dataset):\n",
        "        \"\"\"\n",
        "        Plots the distribution of classes for a list of clients.\n",
        "        \"\"\"\n",
        "        class_distributions = []\n",
        "\n",
        "        if hasattr(dataset, 'targets'):\n",
        "            get_label = lambda idx: dataset.targets[idx]\n",
        "        elif hasattr(dataset, 'labels'):\n",
        "            get_label = lambda idx: dataset.labels[idx]\n",
        "        else:\n",
        "            get_label = lambda idx: dataset[idx][1]\n",
        "\n",
        "        for client in clients:\n",
        "            indices = client.loader.dataset.indices if isinstance(client.loader.dataset, Subset) else list(range(len(client.loader.dataset)))\n",
        "            labels = [get_label(i) for i in indices]\n",
        "            class_counts = Counter(labels)\n",
        "            class_distributions.append(class_counts)\n",
        "\n",
        "        fig, axes = plt.subplots(nrows=1, ncols=len(clients), figsize=(5 * len(clients), 4))\n",
        "        if len(clients) == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for i, (client, class_counts) in enumerate(zip(clients, class_distributions)):\n",
        "            axes[i].bar(class_counts.keys(), class_counts.values(), color='orange')\n",
        "            axes[i].set_title(f'Client {client.client_id}')\n",
        "            axes[i].set_xlabel('Classes')\n",
        "            axes[i].set_ylabel('Frequency')\n",
        "            axes[i].set_xticks(list(class_counts.keys()))\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" CLIENT CREATION \"\"\"\n",
        "def create_clients(data, K, Nc=None, flag = \"iid\"):   ### data is either train_data or tot_data\n",
        "    \"\"\"\n",
        "    Splits the dataset into K subsets, either IID or non-IID, and returns both the client datasets and their corresponding DataLoaders\n",
        "    \"\"\"\n",
        "    if flag == \"iid\":     # We want iid\n",
        "      client_datasets = iid_shard(data, K)     # Split the training data into K clients\n",
        "\n",
        "    else:    # We want non iid\n",
        "      client_datasets = noniid_shard(data, K, Nc)\n",
        "\n",
        "    client_loaders = {}    ### Creation of a DataLoader for each client\n",
        "\n",
        "    for client_id, subset in client_datasets.items():\n",
        "        loader = DataLoader(subset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        client_loaders[client_id] = loader       # The clients id are the indices+1\n",
        "    return client_datasets, client_loaders\n",
        "\n",
        "\n",
        "def create_client_objects(data, K, Nc=None, flag=\"iid\", batch_size=64, verbose=True):\n",
        "    \"\"\"\n",
        "    Creates and returns a list of Client objects, each containing its own dataset and DataLoader, ready for use in federated learning\n",
        "    \"\"\"\n",
        "\n",
        "    # === CHECK ===\n",
        "    if flag == \"non-iid\" and Nc is None:\n",
        "        raise ValueError(\"Nc must be set when flag='non-iid'\")\n",
        "\n",
        "    client_datasets, client_loaders = create_clients(data, K, Nc, flag=flag)\n",
        "    clients = []\n",
        "    for client_id in client_datasets.keys():\n",
        "        clients.append(Client(client_id, client_datasets[client_id], client_loaders[client_id]))\n",
        "\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Clients and their sample sizes:\")\n",
        "        for client in clients:\n",
        "            print(f\"Client {client.client_id}: {client.num_samples()} samples\")\n",
        "\n",
        "    return clients\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5007e58",
      "metadata": {
        "id": "e5007e58"
      },
      "source": [
        "## Server model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adcf5d52",
      "metadata": {
        "id": "adcf5d52"
      },
      "outputs": [],
      "source": [
        "\"\"\" MODEL EVALUATION \"\"\"\n",
        "def evaluate_model(model, data_loader, criterion):\n",
        "    \"\"\"\n",
        "    The evaluate_model function computes the average loss and accuracy of a model on a dataset without updating its weights.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_corrects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            total_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader.dataset)\n",
        "    avg_acc = total_corrects.double() / len(data_loader.dataset)\n",
        "\n",
        "    return avg_loss, avg_acc.item()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "037d0b42",
      "metadata": {
        "id": "037d0b42"
      },
      "outputs": [],
      "source": [
        "\"\"\" CHECKPOINT FUNCTION \"\"\"\n",
        "def save_checkpoint(model, epoch, train_losses, train_accuracies,\n",
        "                    val_test_losses, val_test_accuracies, best_acc, best_loss, best_model_wts, path):\n",
        "    \"\"\"\n",
        "    The save_checkpoint function saves the model’s state, training/validation metrics,\n",
        "    and best performance to a specified file path.\n",
        "    \"\"\"\n",
        "    dir_name = os.path.dirname(path)\n",
        "    if dir_name:\n",
        "        os.makedirs(dir_name, exist_ok=True)\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'train_losses': train_losses,\n",
        "        'train_accuracies': train_accuracies,\n",
        "        'val_test_losses': val_test_losses,\n",
        "        'val_test_accuracies': val_test_accuracies,\n",
        "        'best_acc': best_acc,\n",
        "        'best_loss': best_loss,\n",
        "        'best_model_state_dict': best_model_wts\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "\n",
        "\n",
        "\n",
        "def init_checkpoint(model, path=None, device='cpu', verbose = True):\n",
        "    \"\"\"\n",
        "    Initialize a checkpoint. If path is None, create default checkpoint with empty/default values.\n",
        "    If path is given and file exists, load it.\n",
        "    Returns: start_epoch, best_acc, best_loss, train_losses, train_accuracies, val_test_losses, val_test_accuracies, checkpoint_path, best_model_wts\n",
        "    \"\"\"\n",
        "    if path is None:\n",
        "        # default path\n",
        "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "        path = \"checkpoints/latest.pth\"\n",
        "        if verbose:\n",
        "            print(f\"Initializing new checkpoint at {path}\")\n",
        "        # save default empty checkpoint\n",
        "        checkpoint = {\n",
        "            'epoch': 1,\n",
        "            'best_acc': 0.0,\n",
        "            'best_loss': 1e10,\n",
        "            'train_losses': [],\n",
        "            'train_accuracies': [],\n",
        "            'val_test_losses': [],\n",
        "            'val_test_accuracies': [],\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'best_model_state_dict': copy.deepcopy(model.state_dict())\n",
        "        }\n",
        "        torch.save(checkpoint, path)\n",
        "        return 1, 0.0, 1e10, [], [], [], [], path, copy.deepcopy(model.state_dict())\n",
        "\n",
        "    else:\n",
        "        # load existing checkpoint\n",
        "        if not os.path.isfile(path):\n",
        "            raise FileNotFoundError(f\"Checkpoint file {path} does not exist.\")\n",
        "\n",
        "        print(f\"Loading checkpoint from {path}\")\n",
        "        checkpoint = torch.load(path, map_location=device, weights_only=False)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        best_model_wts = checkpoint.get('best_model_state_dict', copy.deepcopy(model.state_dict()))\n",
        "        return (checkpoint['epoch'],\n",
        "                checkpoint.get('best_acc', 0.0),\n",
        "                checkpoint.get('best_loss', 1e10),\n",
        "                checkpoint.get('train_losses', []),\n",
        "                checkpoint.get('train_accuracies', []),\n",
        "                checkpoint.get('val_test_losses', []),\n",
        "                checkpoint.get('val_test_accuracies', []),\n",
        "                path,\n",
        "                best_model_wts )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17af90e4",
      "metadata": {
        "id": "17af90e4"
      },
      "source": [
        "## Train client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0823dc7",
      "metadata": {
        "id": "a0823dc7"
      },
      "outputs": [],
      "source": [
        "\"\"\" TRAINING FUNCTION \"\"\"\n",
        "def train_model_client(model, criterion, optimizer, scheduler, client: Client, J=5, device=None):    ### Single client\n",
        "    \"\"\"\n",
        "    Trains a model locally on a single client for J epochs.\n",
        "    Performs forward pass, computes loss, backpropagation, and updates weights using the given optimizer.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    train_loader = client.loader\n",
        "\n",
        "    # ========== TRAINING ==========\n",
        "    for j in range(J):   # \"inner\" loop\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            train_corrects += torch.sum(preds == labels).item()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        epoch_train_loss = train_loss / len(train_loader.dataset)\n",
        "        epoch_train_acc = train_corrects / len(train_loader.dataset)\n",
        "\n",
        "    return epoch_train_loss, epoch_train_acc    # Return the last performances\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e886bc0",
      "metadata": {
        "id": "8e886bc0"
      },
      "source": [
        "## Mask & Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c93c222f",
      "metadata": {
        "id": "c93c222f"
      },
      "outputs": [],
      "source": [
        "\"\"\" MASK COMPUTATION & CALIBRATION \"\"\"\n",
        "\n",
        "def compute_fisher_scores(client, model, device):\n",
        "    \"\"\"\n",
        "    Compute diagonal Fisher Information scores for a client using client.loader.\n",
        "    Stores results in client.fisher_scores.\n",
        "    \"\"\"\n",
        "    model.eval()  # set model to evaluation mode\n",
        "\n",
        "    # Initialize Fisher scores if not already present\n",
        "    if not hasattr(client, 'fisher_scores') or client.fisher_scores is None:\n",
        "        client.fisher_scores = {\n",
        "            name: torch.zeros_like(param, device=\"cpu\")\n",
        "            for name, param in model.named_parameters() if param.requires_grad\n",
        "        }\n",
        "\n",
        "    num_batches = 0  # counter for normalization\n",
        "\n",
        "    # Iterate over client data\n",
        "    for inputs, labels in client.loader:\n",
        "        num_batches += 1\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        model.zero_grad()  # reset gradients\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute log-probabilities\n",
        "        log_probs = F.log_softmax(outputs, dim=1)\n",
        "\n",
        "        # Sample labels from the predicted distribution: y ~ p(y|x)\n",
        "        sampled_y = torch.multinomial(log_probs.exp(), num_samples=1).squeeze(-1)\n",
        "\n",
        "        # Compute NLL loss on sampled labels and backpropagate\n",
        "        loss = F.nll_loss(log_probs, sampled_y, reduction=\"mean\")\n",
        "        loss.backward()\n",
        "\n",
        "        # Accumulate squared gradients for each parameter\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                client.fisher_scores[name] += param.grad.detach().cpu() ** 2\n",
        "\n",
        "    # Normalize Fisher scores by number of batches\n",
        "    for name in client.fisher_scores.keys():\n",
        "        client.fisher_scores[name] /= num_batches\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def calibrate_mask_for_client(client, model, device, R=5, final_sparsity=0.9, lr=0.005, weight_decay=0.00005 , keep=\"least\"):\n",
        "    # Initial Mask: all 1 (uint8) only for trainable parameters\n",
        "    mask = {name: torch.ones_like(param, device=\"cpu\", dtype=torch.uint8)\n",
        "            for name, param in model.named_parameters() if param.requires_grad}\n",
        "    client.mask = mask  # RAM\n",
        "\n",
        "    # Evaluate fisher e uploading mask R times\n",
        "    for r in range(1, R + 1):\n",
        "        current_density = (1 - final_sparsity) ** (r / R)\n",
        "        current_sparsity = 1 - current_density\n",
        "        print(f\"[Round = {r}] Sparsity = {current_sparsity:.4f}\")\n",
        "\n",
        "        compute_fisher_scores(client, model, device)\n",
        "        fisher_scores = client.fisher_scores  # dict {name: tensor float}\n",
        "\n",
        "        # --- Flatten all scores ---\n",
        "        all_scores = torch.cat([score.view(-1) for score in fisher_scores.values()])\n",
        "\n",
        "        # --- Evaluate treshold with topk ---\n",
        "        num_keep_global = int(len(all_scores) * current_density)\n",
        "        num_keep_global = max(1, min(num_keep_global, len(all_scores)))\n",
        "\n",
        "        if keep == \"least\":\n",
        "            # take the least important weights\n",
        "            _, idx = torch.topk(all_scores, k=num_keep_global, largest=False)\n",
        "        elif keep == \"most\":\n",
        "            # take the best important weights\n",
        "            _, idx = torch.topk(all_scores, k=num_keep_global, largest=True)\n",
        "        elif keep == \"random\":\n",
        "            idx = torch.randperm(len(all_scores))[:num_keep_global]\n",
        "        else:\n",
        "            raise ValueError(\"keep must be 'least', 'most', or 'random'\")\n",
        "\n",
        "        # build global_keep\n",
        "        global_keep = torch.zeros_like(all_scores, dtype=torch.bool)\n",
        "        global_keep[idx] = True\n",
        "\n",
        "        # --- Redistribute the threshold layer by layer ---\n",
        "        new_mask = {}\n",
        "        start = 0\n",
        "        for name, score in fisher_scores.items():\n",
        "            numel = score.numel()\n",
        "            keep_tensor = global_keep[start:start+numel].view_as(score)\n",
        "            new_mask[name] = (keep_tensor.to(torch.uint8) * client.mask[name])\n",
        "            start += numel\n",
        "\n",
        "        # upload current mask\n",
        "        client.mask = new_mask\n",
        "        total_ones = sum(mask.sum().item() for mask in client.mask.values())\n",
        "        #print(f\"total active parameters: {total_ones}\")\n",
        "        total_params = sum(mask.numel() for mask in client.mask.values())\n",
        "        perc_active = 100 * total_ones / total_params\n",
        "        print(f\"Parametri attivi: {total_ones}/{total_params} ({perc_active:.2f}%)\")\n",
        "\n",
        "        # mini-training with SparseSGD + current mask\n",
        "        param_to_name = {id(param): n for n, param in model.named_parameters()}\n",
        "        optimizer = SparseSGD(model.parameters(), lr=lr, weight_decay=weight_decay, mask_dict=client.mask, param_to_name=param_to_name)\n",
        "        local_model = copy.deepcopy(model).to(device)\n",
        "        train_model_client(local_model, criterion=nn.CrossEntropyLoss(), optimizer=optimizer,\n",
        "                           scheduler=None, client=client, J=1, device=device)\n",
        "\n",
        "    # End calibrazione: free memory by fisher scores\n",
        "    client.fisher_scores = None\n",
        "\n",
        "    # compress mask\n",
        "    client.compress_mask()\n",
        "\n",
        "\n",
        "\n",
        "def calibrate_all_clients(clients, global_model, device, R=5, final_sparsity=0.9, lr=0.005, weight_decay=0.00005, keep =\"least\"):\n",
        "    \"\"\"\n",
        "    Apply mask calibration for all clients.\n",
        "    \"\"\"\n",
        "    for client in clients:\n",
        "        calibrate_mask_for_client(client, global_model, device, R=R, final_sparsity=final_sparsity, lr=lr, weight_decay=weight_decay, keep=keep)\n",
        "        print(f\"Client {client.client_id} -> mask calibrated (sparsity={final_sparsity})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9fdaad5",
      "metadata": {
        "id": "e9fdaad5"
      },
      "source": [
        "## SparseSGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e79dce9",
      "metadata": {
        "id": "0e79dce9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import SGD\n",
        "\n",
        "class SparseSGD(torch.optim.SGD):\n",
        "    def __init__(self, params, lr=0.01, momentum=0, weight_decay=0, mask_dict=None, param_to_name=None):\n",
        "        super().__init__(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "        self.mask_dict = mask_dict  # {name: mask_tensor su CPU}\n",
        "        self.param_to_name = param_to_name or {}\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        # Before super().step: gradients ubdate following the mask\n",
        "        if self.mask_dict is not None:\n",
        "            for group in self.param_groups:\n",
        "                for p in group['params']:\n",
        "                    if p.grad is None:\n",
        "                        continue\n",
        "                    name = self.param_to_name.get(id(p), None)\n",
        "                    if name is not None and name in self.mask_dict:\n",
        "                        mask = self.mask_dict[name].to(p.device)\n",
        "                        # applying the gradient mask (block update of masked-out weights)\n",
        "                        p.grad.mul_(mask.to(dtype=p.grad.dtype))     # CORRESPONDS TO: d_p = d_p * mask\n",
        "\n",
        "        # SGD steps(momentum, weight_decay, ecc.)\n",
        "        super().step(closure)\n",
        "\n",
        "        return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6cb5b9d",
      "metadata": {
        "id": "d6cb5b9d"
      },
      "source": [
        "## Main Training FL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ee6186f",
      "metadata": {
        "id": "9ee6186f"
      },
      "outputs": [],
      "source": [
        "def train_test_model_FL(global_model, criterion, LR, MOMENTUM, WEIGHT_DECAY, T_MAX, clients, val_test_loader,\n",
        "                        num_epochs, C, J, checkpoint_path=None, checkpoints=True, verbose=True, use_sparse=False):\n",
        "    \"\"\"\n",
        "    Federated Learning training loop with FedAvg aggregation and optional SparseSGD.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    val_test_loss_s, val_test_acc_s, avg_train_loss_s, avg_train_acc_s = [], [], [], []\n",
        "\n",
        "    # copy of global weights\n",
        "    fed_model_weights = {k: v.clone().detach() for k, v in global_model.state_dict().items()}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        round_train_losses = []\n",
        "        round_train_accs = []\n",
        "\n",
        "        if verbose or epoch == 0 or (epoch + 1) % 5 == 0:\n",
        "            print(f\"\\n--- Federated Round {epoch+1}/{num_epochs} ---\")\n",
        "\n",
        "        # sampling of clients\n",
        "        num_clients = max(1, int(C * len(clients)))\n",
        "        selected_clients = np.random.choice(clients, num_clients, replace=False)\n",
        "\n",
        "        local_weights = []\n",
        "        local_sizes = []\n",
        "\n",
        "        for client in selected_clients:\n",
        "\n",
        "            local_model = copy.deepcopy(global_model)\n",
        "            local_model.load_state_dict(fed_model_weights)\n",
        "            local_model.to(device)\n",
        "\n",
        "            #  mapping param->name\n",
        "            param_to_name = {id(param): name for name, param in local_model.named_parameters()}\n",
        "\n",
        "            if use_sparse:\n",
        "                client.decompress_mask()\n",
        "                optimizer = SparseSGD(local_model.parameters(),\n",
        "                                      lr=LR,\n",
        "                                      momentum=MOMENTUM,\n",
        "                                      weight_decay=WEIGHT_DECAY,\n",
        "                                      mask_dict=client.mask,\n",
        "                                      param_to_name=param_to_name)\n",
        "            else:\n",
        "                optimizer = torch.optim.SGD(local_model.parameters(),\n",
        "                                            lr=LR,\n",
        "                                            momentum=MOMENTUM,\n",
        "                                            weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_MAX) if T_MAX else None\n",
        "\n",
        "            # Training local model\n",
        "            train_loss, train_acc = train_model_client(local_model, criterion, optimizer, scheduler, client, J, device=device)\n",
        "            round_train_losses.append(train_loss)\n",
        "            round_train_accs.append(train_acc)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Client {client.client_id} -> Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "            # collecting weights of FedAvg\n",
        "            local_weights.append({k: v.cpu().detach() for k, v in local_model.state_dict().items()})\n",
        "            local_sizes.append(client.num_samples())\n",
        "\n",
        "            # free GP memoryU\n",
        "            del local_model, optimizer, scheduler\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # FedAvg aggregation\n",
        "        total_samples = sum(local_sizes)\n",
        "        fed_model_weights = {}\n",
        "        for key in local_weights[0].keys():\n",
        "            fed_model_weights[key] = sum([local_weights[i][key] * (local_sizes[i]/total_samples)\n",
        "                                          for i in range(len(local_weights))])\n",
        "\n",
        "        # Update global model\n",
        "        global_model.load_state_dict(fed_model_weights)\n",
        "\n",
        "        # GLobal evaluation\n",
        "        avg_train_loss = np.mean(round_train_losses)\n",
        "        avg_train_acc = np.mean(round_train_accs)\n",
        "        val_test_loss, val_test_acc = evaluate_model(global_model, val_test_loader, criterion)\n",
        "        if verbose or epoch == 0 or (epoch + 1) % 5 == 0:\n",
        "            print(f\"Round {epoch+1} -> Avg Train Loss: {avg_train_loss:.4f}, Avg Train Acc: {avg_train_acc:.4f}\")\n",
        "            print(f\"Round {epoch+1} -> Val Loss: {val_test_loss:.4f}, Val Acc: {val_test_acc:.4f}\")\n",
        "\n",
        "        val_test_loss_s.append(val_test_loss)\n",
        "        val_test_acc_s.append(val_test_acc)\n",
        "        avg_train_loss_s.append(avg_train_loss)\n",
        "        avg_train_acc_s.append(avg_train_acc)\n",
        "\n",
        "        # Save checkpoint\n",
        "        if checkpoints and checkpoint_path:\n",
        "            torch.save(global_model.state_dict(), f\"{checkpoint_path}_round{epoch+1}.pth\")\n",
        "\n",
        "    return global_model, val_test_loss_s, val_test_acc_s, avg_train_loss_s, avg_train_acc_s\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59a960bf",
      "metadata": {
        "id": "59a960bf"
      },
      "outputs": [],
      "source": [
        "def train_test_model_FL_global(global_model, criterion, LR, MOMENTUM, WEIGHT_DECAY, T_MAX, clients, val_test_loader,\n",
        "                               num_epochs, C, J, mask_global=None,\n",
        "                               checkpoint_path=None, checkpoints=True, verbose=True, use_sparse=False):\n",
        "    \"\"\"\n",
        "    Federated Learning training loop with FedAvg aggregation and optional SparseSGD.\n",
        "    - If use_sparse=True and mask_global is not None, the global mask is used for all clients.\n",
        "    - If use_sparse=True and mask_global is None, each client uses its own mask.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    val_test_loss_s, val_test_acc_s, avg_train_loss_s, avg_train_acc_s = [], [], [], []\n",
        "\n",
        "    # copy of the initial model weights\n",
        "    fed_model_weights = {k: v.clone().detach() for k, v in global_model.state_dict().items()}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        round_train_losses = []\n",
        "        round_train_accs = []\n",
        "\n",
        "        if verbose or epoch == 0 or (epoch + 1) % 5 == 0:\n",
        "            print(f\"\\n--- Federated Round {epoch+1}/{num_epochs} ---\")\n",
        "\n",
        "        # Sample a fraction of clients\n",
        "        num_clients = max(1, int(C * len(clients)))\n",
        "        selected_clients = np.random.choice(clients, num_clients, replace=False)\n",
        "\n",
        "        local_weights = []\n",
        "        local_sizes = []\n",
        "\n",
        "        for client in selected_clients:\n",
        "            # Copy the global model for local training\n",
        "            local_model = copy.deepcopy(global_model)\n",
        "            local_model.load_state_dict(fed_model_weights)\n",
        "            local_model.to(device)\n",
        "\n",
        "            # Build the param->name mapping\n",
        "            param_to_name = {id(param): name for name, param in local_model.named_parameters()}\n",
        "\n",
        "            if use_sparse:\n",
        "                if mask_global is not None:\n",
        "                    # All clients use the global mask\n",
        "                    optimizer = SparseSGD(local_model.parameters(),\n",
        "                                          lr=LR,\n",
        "                                          momentum=MOMENTUM,\n",
        "                                          weight_decay=WEIGHT_DECAY,\n",
        "                                          mask_dict=mask_global,\n",
        "                                          param_to_name=param_to_name)\n",
        "                else:\n",
        "                    # Each client uses its own mask\n",
        "                    client.decompress_mask()\n",
        "                    optimizer = SparseSGD(local_model.parameters(),\n",
        "                                          lr=LR,\n",
        "                                          momentum=MOMENTUM,\n",
        "                                          weight_decay=WEIGHT_DECAY,\n",
        "                                          mask_dict=client.mask,\n",
        "                                          param_to_name=param_to_name)\n",
        "            else:\n",
        "                optimizer = torch.optim.SGD(local_model.parameters(),\n",
        "                                            lr=LR,\n",
        "                                            momentum=MOMENTUM,\n",
        "                                            weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_MAX) if T_MAX else None\n",
        "\n",
        "            # Train the local model on the client\n",
        "            train_loss, train_acc = train_model_client(local_model, criterion, optimizer, scheduler, client, J, device=device)\n",
        "            round_train_losses.append(train_loss)\n",
        "            round_train_accs.append(train_acc)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Client {client.client_id} -> Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "            # Collect local weights for FedAvg\n",
        "            local_weights.append({k: v.cpu().detach() for k, v in local_model.state_dict().items()})\n",
        "            local_sizes.append(client.num_samples())\n",
        "\n",
        "            # Free GPU memory\n",
        "            del local_model, optimizer, scheduler\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # FedAvg aggregation\n",
        "        total_samples = sum(local_sizes)\n",
        "        fed_model_weights = {}\n",
        "        for key in local_weights[0].keys():\n",
        "            fed_model_weights[key] = sum([local_weights[i][key] * (local_sizes[i]/total_samples)\n",
        "                                          for i in range(len(local_weights))])\n",
        "\n",
        "        # Update the global model\n",
        "        global_model.load_state_dict(fed_model_weights)\n",
        "\n",
        "        # Global evaluation\n",
        "        avg_train_loss = np.mean(round_train_losses)\n",
        "        avg_train_acc = np.mean(round_train_accs)\n",
        "        val_test_loss, val_test_acc = evaluate_model(global_model, val_test_loader, criterion)\n",
        "        if verbose or epoch == 0 or (epoch + 1) % 5 == 0:\n",
        "            print(f\"Round {epoch+1} -> Avg Train Loss: {avg_train_loss:.4f}, Avg Train Acc: {avg_train_acc:.4f}\")\n",
        "            print(f\"Round {epoch+1} -> Val Loss: {val_test_loss:.4f}, Val Acc: {val_test_acc:.4f}\")\n",
        "\n",
        "        val_test_loss_s.append(val_test_loss)\n",
        "        val_test_acc_s.append(val_test_acc)\n",
        "        avg_train_loss_s.append(avg_train_loss)\n",
        "        avg_train_acc_s.append(avg_train_acc)\n",
        "\n",
        "        # Save checkpoint\n",
        "        if checkpoints and checkpoint_path:\n",
        "            torch.save(global_model.state_dict(), f\"{checkpoint_path}_round{epoch+1}.pth\")\n",
        "\n",
        "    return global_model, val_test_loss_s, val_test_acc_s, avg_train_loss_s, avg_train_acc_s\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e889421d",
      "metadata": {
        "id": "e889421d"
      },
      "source": [
        "# _________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "249e5ee2-1eda-47b4-8a72-de2f6078c105",
      "metadata": {
        "id": "249e5ee2-1eda-47b4-8a72-de2f6078c105"
      },
      "source": [
        "# (1) Head Pre-Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dced52ff-bfae-43f8-971c-39e3d3f41c03",
      "metadata": {
        "id": "dced52ff-bfae-43f8-971c-39e3d3f41c03"
      },
      "outputs": [],
      "source": [
        "set_seeds(123)\n",
        "checkpoint_path = None\n",
        "\n",
        "\n",
        "############################\n",
        "# === PARAMETERS TO SET ===\n",
        "FLAG = \"iid\"    # 'niid'\n",
        "Nc = None       # {1,5,10,50}\n",
        "J = 4           # {4,8,16}\n",
        "############################\n",
        "\n",
        "\n",
        "# === GENERAL PARAMETERS ===\n",
        "N_EP = 50   #100    # global rounds (out)\n",
        "K = #100\n",
        "C = 0.1\n",
        "LR = 0.003\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0.0001\n",
        "T_MAX = J   # Same as client rounds (in)\n",
        "VERBOSE = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7c48922-cccd-475d-8646-881d4eac851f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7c48922-cccd-475d-8646-881d4eac851f",
        "outputId": "9081e0af-efc4-4e2f-9c0c-9518a577867f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/dino/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dino_deitsmall16_pretrain.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 82.7M/82.7M [00:04<00:00, 19.2MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Federated Round 1/100 ---\n",
            "Round 1 -> Avg Train Loss: 0.0209, Avg Train Acc: 1.0000\n",
            "Round 1 -> Val Loss: 2.8739, Val Acc: 0.3264\n",
            "\n",
            "--- Federated Round 5/100 ---\n",
            "Round 5 -> Avg Train Loss: 0.0246, Avg Train Acc: 1.0000\n",
            "Round 5 -> Val Loss: 1.6327, Val Acc: 0.5661\n",
            "\n",
            "--- Federated Round 10/100 ---\n",
            "Round 10 -> Avg Train Loss: 0.0221, Avg Train Acc: 1.0000\n",
            "Round 10 -> Val Loss: 1.3901, Val Acc: 0.6227\n",
            "\n",
            "--- Federated Round 15/100 ---\n",
            "Round 15 -> Avg Train Loss: 0.0203, Avg Train Acc: 1.0000\n",
            "Round 15 -> Val Loss: 1.2954, Val Acc: 0.6479\n",
            "\n",
            "--- Federated Round 20/100 ---\n",
            "Round 20 -> Avg Train Loss: 0.0195, Avg Train Acc: 1.0000\n",
            "Round 20 -> Val Loss: 1.2607, Val Acc: 0.6572\n",
            "\n",
            "--- Federated Round 25/100 ---\n",
            "Round 25 -> Avg Train Loss: 0.0196, Avg Train Acc: 1.0000\n",
            "Round 25 -> Val Loss: 1.2338, Val Acc: 0.6669\n",
            "\n",
            "--- Federated Round 30/100 ---\n",
            "Round 30 -> Avg Train Loss: 0.0184, Avg Train Acc: 1.0000\n",
            "Round 30 -> Val Loss: 1.2111, Val Acc: 0.6724\n",
            "\n",
            "--- Federated Round 35/100 ---\n",
            "Round 35 -> Avg Train Loss: 0.0182, Avg Train Acc: 1.0000\n",
            "Round 35 -> Val Loss: 1.1947, Val Acc: 0.6756\n",
            "\n",
            "--- Federated Round 40/100 ---\n",
            "Round 40 -> Avg Train Loss: 0.0176, Avg Train Acc: 1.0000\n",
            "Round 40 -> Val Loss: 1.1833, Val Acc: 0.6836\n",
            "\n",
            "--- Federated Round 45/100 ---\n",
            "Round 45 -> Avg Train Loss: 0.0180, Avg Train Acc: 1.0000\n",
            "Round 45 -> Val Loss: 1.1705, Val Acc: 0.6890\n",
            "\n",
            "--- Federated Round 50/100 ---\n",
            "Round 50 -> Avg Train Loss: 0.0169, Avg Train Acc: 1.0000\n",
            "Round 50 -> Val Loss: 1.1695, Val Acc: 0.6862\n",
            "\n",
            "--- Federated Round 55/100 ---\n",
            "Round 55 -> Avg Train Loss: 0.0165, Avg Train Acc: 1.0000\n",
            "Round 55 -> Val Loss: 1.1793, Val Acc: 0.6847\n",
            "\n",
            "--- Federated Round 60/100 ---\n",
            "Round 60 -> Avg Train Loss: 0.0169, Avg Train Acc: 1.0000\n",
            "Round 60 -> Val Loss: 1.1839, Val Acc: 0.6876\n",
            "\n",
            "--- Federated Round 65/100 ---\n",
            "Round 65 -> Avg Train Loss: 0.0164, Avg Train Acc: 1.0000\n",
            "Round 65 -> Val Loss: 1.1767, Val Acc: 0.6899\n",
            "\n",
            "--- Federated Round 70/100 ---\n",
            "Round 70 -> Avg Train Loss: 0.0163, Avg Train Acc: 1.0000\n",
            "Round 70 -> Val Loss: 1.1699, Val Acc: 0.6901\n",
            "\n",
            "--- Federated Round 75/100 ---\n",
            "Round 75 -> Avg Train Loss: 0.0167, Avg Train Acc: 1.0000\n",
            "Round 75 -> Val Loss: 1.1704, Val Acc: 0.6938\n",
            "\n",
            "--- Federated Round 80/100 ---\n",
            "Round 80 -> Avg Train Loss: 0.0158, Avg Train Acc: 1.0000\n",
            "Round 80 -> Val Loss: 1.1833, Val Acc: 0.6925\n",
            "\n",
            "--- Federated Round 85/100 ---\n",
            "Round 85 -> Avg Train Loss: 0.0157, Avg Train Acc: 1.0000\n",
            "Round 85 -> Val Loss: 1.1827, Val Acc: 0.6945\n",
            "\n",
            "--- Federated Round 90/100 ---\n",
            "Round 90 -> Avg Train Loss: 0.0157, Avg Train Acc: 1.0000\n",
            "Round 90 -> Val Loss: 1.1910, Val Acc: 0.6921\n",
            "\n",
            "--- Federated Round 95/100 ---\n",
            "Round 95 -> Avg Train Loss: 0.0155, Avg Train Acc: 1.0000\n",
            "Round 95 -> Val Loss: 1.1882, Val Acc: 0.6916\n",
            "\n",
            "--- Federated Round 100/100 ---\n",
            "Round 100 -> Avg Train Loss: 0.0153, Avg Train Acc: 1.0000\n",
            "Round 100 -> Val Loss: 1.1986, Val Acc: 0.6896\n"
          ]
        }
      ],
      "source": [
        "set_seeds(123)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# === MODEL BACKBONE ===\n",
        "dino_vits16 = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
        "dino_vits16 = dino_vits16.to(device)\n",
        "\n",
        "# Freeze backbone parameters\n",
        "for param in dino_vits16.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# === CREATE GLOBAL MODEL (trainable head) ===\n",
        "global_model = DINOWithHead(dino_vits16, num_classes=100).to(device)\n",
        "for param in global_model.head.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "global_model.backbone.eval()   # Backbone only evaluation\n",
        "global_model.head.train()   # Head trianing\n",
        "\n",
        "# === LOSS FUNCTION ===\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "# === TRAINING ====\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "# === CREATE CLIENTS ===\n",
        "clients = create_client_objects(\n",
        "data=tot_train_data,     # USING TOT TRAINING DATA\n",
        "K=K,\n",
        "Nc=Nc,\n",
        "flag=FLAG,\n",
        "batch_size=BATCH_SIZE,\n",
        "verbose=VERBOSE\n",
        ")\n",
        "\n",
        "\n",
        "# === CALL FL TRAINING ===\n",
        "best_global_model_iid, test_losses_iid, test_accuracies_iid, avg_train_losses_iid, avg_train_accuracies_iid = train_test_model_FL(\n",
        "        global_model=global_model,\n",
        "        criterion=criterion,\n",
        "        LR=LR,\n",
        "        MOMENTUM=MOMENTUM,\n",
        "        WEIGHT_DECAY=WEIGHT_DECAY,\n",
        "        T_MAX=T_MAX,\n",
        "        clients=clients,\n",
        "        val_test_loader=test_loader,\n",
        "        num_epochs = N_EP,\n",
        "        C=C,\n",
        "        J=J,\n",
        "        checkpoint_path=checkpoint_path,\n",
        "        checkpoints=True,\n",
        "        verbose=VERBOSE,\n",
        "        use_sparse=False    # Using standard SGD\n",
        "    )\n",
        "end_time = time.time()\n",
        "training_time = round(end_time - start_time, 2)\n",
        "\n",
        "# === SAVING RESULTS ===\n",
        "results_FL_head = {\n",
        "        \"flag\": FLAG,\n",
        "        \"J\": J,\n",
        "        \"Nc\": Nc,\n",
        "        \"epochs\": N_EP,\n",
        "        \"avg_train_losses\": avg_train_losses_iid,\n",
        "        \"avg_train_accuracies\": avg_train_accuracies_iid,\n",
        "        \"test_losses\": test_losses_iid,\n",
        "        \"test_accuracies\": test_accuracies_iid,\n",
        "        \"training_time_sec\": training_time\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a02902d9-efe7-49b0-ba45-e84136f97e56",
      "metadata": {
        "id": "a02902d9-efe7-49b0-ba45-e84136f97e56"
      },
      "outputs": [],
      "source": [
        "# === SAVING JSON ===\n",
        "json_filename = f\"results_FL_HEAD_{FLAG}_J{J}_Nc{Nc}.json\"\n",
        "with open(json_filename, 'w') as f:\n",
        "    json.dump(results_FL_head, f, indent=2)\n",
        "\n",
        "# === SAVING CLIENTS ===\n",
        "clients_filename = f\"clients_FL_HEAD_{FLAG}_J{J}_Nc{Nc}.pkl\"\n",
        "with open(clients_filename, 'wb') as f:\n",
        "    pickle.dump(clients, f)\n",
        "\n",
        "# === SAVING WEIGHTS ===\n",
        "model_filename = f\"global_model_FL_HEAD_{FLAG}_J{J}_Nc{Nc}.pth\"\n",
        "torch.save(global_model.state_dict(), model_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "563de9fb",
      "metadata": {
        "id": "563de9fb"
      },
      "outputs": [],
      "source": [
        "\"\"\" FEDERATED LEARNING RESULTS \"\"\"\n",
        "\n",
        "# === Experiment settings ===\n",
        "# List of JSON files with results from different experiments\n",
        "results_files = glob.glob(\"results_FL_HEAD_*.json\")  # all saved files\n",
        "baseline_file = \"results_central_BEST.json\"         # centralized baseline\n",
        "\n",
        "# --- Load baseline ---\n",
        "with open(baseline_file, 'r') as f:\n",
        "    baseline = json.load(f)\n",
        "\n",
        "# --- Build summary list ---\n",
        "summary_list = []\n",
        "\n",
        "for fpath in results_files:\n",
        "    with open(fpath, 'r') as f:\n",
        "        res = json.load(f)\n",
        "    summary_list.append({\n",
        "        'flag': res.get('flag'),\n",
        "        'J': res.get('J'),\n",
        "        'Nc': res.get('Nc'),\n",
        "        'avg_train_acc': res.get('avg_train_accuracies')[-1] if res.get('avg_train_accuracies') else None,\n",
        "        'test_acc': res.get('test_accuracies')[-1] if res.get('test_accuracies') else None,\n",
        "        'test_accuracies': res.get('test_accuracies'),  # save all accuracies for line plot\n",
        "        'training_time_sec': res.get('training_time_sec')\n",
        "    })\n",
        "\n",
        "# --- Add centralized baseline ---\n",
        "summary_list.append({\n",
        "    'flag': 'central',\n",
        "    'J': 0,\n",
        "    'Nc': 0,\n",
        "    'avg_train_acc': baseline.get('avg_train_accuracies')[-1] if baseline.get('avg_train_accuracies') else None,\n",
        "    'test_acc': baseline.get('test_accuracies')[-1] if baseline.get('test_accuracies') else None,\n",
        "    'test_accuracies': baseline.get('test_accuracies'),\n",
        "    'training_time_sec': baseline.get('training_time_sec')\n",
        "})\n",
        "\n",
        "# --- Create DataFrame ---\n",
        "df_summary = pd.DataFrame(summary_list)\n",
        "print(df_summary)\n",
        "\n",
        "# === Definition of the two macro-groups ===\n",
        "group1 = [\n",
        "    {'flag':'iid', 'J':4, 'Nc':None},\n",
        "    {'flag':'niid', 'J':4, 'Nc':1},\n",
        "    {'flag':'niid', 'J':4, 'Nc':5},\n",
        "    {'flag':'niid', 'J':4, 'Nc':10},\n",
        "    {'flag':'niid', 'J':4, 'Nc':50},\n",
        "]\n",
        "\n",
        "group2 = [\n",
        "    {'flag':'iid', 'J':4, 'Nc':None},\n",
        "    {'flag':'iid', 'J':8, 'Nc':None},\n",
        "    {'flag':'niid', 'J':4, 'Nc':10},\n",
        "    {'flag':'niid', 'J':8, 'Nc':10},\n",
        "    {'flag':'niid', 'J':16, 'Nc':10},\n",
        "]\n",
        "\n",
        "# === Function for simple bar plot ===\n",
        "def plot_bar(group, title):\n",
        "    labels = []\n",
        "    accuracies = []\n",
        "    colors = []\n",
        "    for combo in group:\n",
        "        subset = df_summary[df_summary['flag']==combo['flag']]\n",
        "        subset = subset[subset['J']==combo['J']]\n",
        "        if combo['Nc'] is not None:\n",
        "            subset = subset[subset['Nc']==combo['Nc']]\n",
        "        if not subset.empty:\n",
        "            # Simple label\n",
        "            label = f\"{combo['flag']} J{combo['J']}\"\n",
        "            if combo['Nc'] is not None:\n",
        "                label += f\" Nc{combo['Nc']}\"\n",
        "            labels.append(label)\n",
        "            accuracies.append(subset['test_acc'].values[0])\n",
        "            colors.append('blue' if combo['flag']=='iid' else 'orange')\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.bar(labels, accuracies, color=colors)\n",
        "    plt.ylabel(\"Test Accuracy\")\n",
        "    plt.title(title)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# === Function for line plot (accuracies across training) ===\n",
        "def plot_accuracy(group, title):\n",
        "    plt.figure(figsize=(10,6))\n",
        "    for combo in group:\n",
        "        subset = df_summary[df_summary['flag']==combo['flag']]\n",
        "        subset = subset[subset['J']==combo['J']]\n",
        "        if combo['Nc'] is not None:\n",
        "            subset = subset[subset['Nc']==combo['Nc']]\n",
        "        if not subset.empty:\n",
        "            acc_list = subset['test_accuracies'].values[0]\n",
        "            plt.plot(range(1,len(acc_list)+1), acc_list, marker='o', label=f\"{combo['flag']} J{combo['J']}\" + (f\" Nc{combo['Nc']}\" if combo['Nc'] is not None else \"\"))\n",
        "\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Test Accuracy\")\n",
        "    plt.title(title)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# === Create plots ===\n",
        "# Bar plots\n",
        "plot_bar(group1, \"Bar Plot - Group 1\")\n",
        "plot_bar(group2, \"Bar Plot - Group 2\")\n",
        "\n",
        "# Line plots accuracy\n",
        "plot_accuracy(group1, \"Test Accuracy Across Training - Group 1\")\n",
        "plot_accuracy(group2, \"Test Accuracy Across Training - Group 2\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
